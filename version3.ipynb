{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a7a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "⚠️  INPUT REQUIREMENTS:\n",
      "- You must include a line starting with 'Topic:'\n",
      "- You must include at least one line starting with 'Questions:'\n",
      "Otherwise, the survey cannot be processed.\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭──────────────────────────────────────────────── Flow Execution ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Starting Flow Execution</span>                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #000080; text-decoration-color: #000080\">SurveyFlow</span>                                                                                               <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #000080; text-decoration-color: #000080\">7d74304a-5991-495a-93ac-8c08d0099b92</span>                                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m───────────────────────────────────────────────\u001b[0m\u001b[34m Flow Execution \u001b[0m\u001b[34m────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34mStarting Flow Execution\u001b[0m                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[37mName: \u001b[0m\u001b[34mSurveyFlow\u001b[0m                                                                                               \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[37mID: \u001b[0m\u001b[34m7d74304a-5991-495a-93ac-8c08d0099b92\u001b[0m                                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n",
      "/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m \n",
      "[2025-05-13 17:05:24][WARNING]: Failed to init knowledge: The CHROMA_OPENAI_API_KEY environment variable is not set.\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────────── Crew Execution Started ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">64849065-21cd-4849-9374-b0dab0b883cb</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────────────\u001b[0m\u001b[36m Crew Execution Started \u001b[0m\u001b[36m────────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m  \u001b[1;36mCrew Execution Started\u001b[0m                                                                                         \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m  \u001b[37mName: \u001b[0m\u001b[36mcrew\u001b[0m                                                                                                     \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m  \u001b[37mID: \u001b[0m\u001b[36m64849065-21cd-4849-9374-b0dab0b883cb\u001b[0m                                                                       \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m                                                                                                                 \u001b[36m│\u001b[0m\n",
       "\u001b[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">🚀 Crew: crew</span>\n",
       "└── <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m🚀 Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       "└── \u001b[1;33m📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">🚀 Crew: crew</span>\n",
       "└── <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "    └── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🤖 Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Survey Content Conversion Agent</span>\n",
       "        \n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m🚀 Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       "└── \u001b[1;33m📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "    └── \u001b[1;32m🤖 Agent: \u001b[0m\u001b[32mSurvey Content Conversion Agent\u001b[0m\n",
       "        \n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSurvey Content Conversion Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mConvert the following survey (provided as raw text) into a structured JSON schema suitable for creating a survey in Qualtrics or similar platforms:\n",
      "Topic: Social Media Usage  Questions: 1. How many hours per day do you spend on social media? 2. What platforms do you use the most? (e.g., Instagram, Twitter, TikTok) 3. Do you feel social media has a positive or negative impact on your mental health? 4. Why do you use social media? (Open text)\n",
      "Make sure your output includes: - A top-level \"title\" field for the survey title - A \"fields\" array, where each element has:\n",
      "  • \"title\": the question text  \n",
      "  • \"type\": the question type (e.g. \"multiple_choice\", \"text_input\")  \n",
      "  • \"options\": a list of answer options (if applicable; omit or set to [] otherwise)\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🤖 Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Survey Content Conversion Agent</span>\n",
       "\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "└── <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">🧠 </span><span style=\"color: #000080; text-decoration-color: #000080\">Thinking...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m🤖 Agent: \u001b[0m\u001b[32mSurvey Content Conversion Agent\u001b[0m\n",
       "\n",
       "\u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       "└── \u001b[1;34m🧠 \u001b[0m\u001b[34mThinking...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">🚀 Crew: crew</span>\n",
       "└── <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Executing Task...</span>\n",
       "    └── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🤖 Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Survey Content Conversion Agent</span>\n",
       "        \n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "        └── <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">❌ LLM Failed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m🚀 Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       "└── \u001b[1;33m📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa\u001b[0m\n",
       "    \u001b[37m   Status: \u001b[0m\u001b[2;33mExecuting Task...\u001b[0m\n",
       "    └── \u001b[1;32m🤖 Agent: \u001b[0m\u001b[32mSurvey Content Conversion Agent\u001b[0m\n",
       "        \n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       "        └── \u001b[1;31m❌ LLM Failed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────────────────────────── LLM Error ───────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">❌ LLM Call Failed</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Error: </span><span style=\"color: #800000; text-decoration-color: #800000\">litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be </span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000\">set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────────────────────────\u001b[0m\u001b[31m LLM Error \u001b[0m\u001b[31m──────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[1;31m❌ LLM Call Failed\u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[37mError: \u001b[0m\u001b[31mlitellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be \u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[31mset either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m Error during LLM call: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[00m\n",
      "\u001b[91m An unknown error occurred. Please check the details below.\u001b[00m\n",
      "\u001b[91m Error details: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">🚀 Crew: crew</span>\n",
       "└── <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa</span>\n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Assigned to: </span><span style=\"color: #800000; text-decoration-color: #800000\">Survey Content Conversion Agent</span>\n",
       "    \n",
       "    <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   Status: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">❌ Failed</span>\n",
       "    └── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🤖 Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Survey Content Conversion Agent</span>\n",
       "        \n",
       "        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Status: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">In Progress</span>\n",
       "        └── <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">❌ LLM Failed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m🚀 Crew: \u001b[0m\u001b[1;36mcrew\u001b[0m\n",
       "└── \u001b[1;31m📋 Task: 11d4637a-d2d8-410a-894d-a696181a71fa\u001b[0m\n",
       "    \u001b[37m   Assigned to: \u001b[0m\u001b[31mSurvey Content Conversion Agent\u001b[0m\n",
       "    \n",
       "    \u001b[37m   Status: \u001b[0m\u001b[1;31m❌ Failed\u001b[0m\n",
       "    └── \u001b[1;32m🤖 Agent: \u001b[0m\u001b[32mSurvey Content Conversion Agent\u001b[0m\n",
       "        \n",
       "        \u001b[37m    Status: \u001b[0m\u001b[1;32mIn Progress\u001b[0m\n",
       "        └── \u001b[1;31m❌ LLM Failed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭───────────────────────────────────────────────── Task Failure ──────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Task Failed</span>                                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #800000; text-decoration-color: #800000\">11d4637a-d2d8-410a-894d-a696181a71fa</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #800000; text-decoration-color: #800000\">Survey Content Conversion Agent</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────────────────────────\u001b[0m\u001b[31m Task Failure \u001b[0m\u001b[31m─────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[1;31mTask Failed\u001b[0m                                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[37mName: \u001b[0m\u001b[31m11d4637a-d2d8-410a-894d-a696181a71fa\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[31mSurvey Content Conversion Agent\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭───────────────────────────────────────────────── Crew Failure ──────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Crew Execution Failed</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #800000; text-decoration-color: #800000\">crew</span>                                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #800000; text-decoration-color: #800000\">64849065-21cd-4849-9374-b0dab0b883cb</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────────────────────────\u001b[0m\u001b[31m Crew Failure \u001b[0m\u001b[31m─────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[1;31mCrew Execution Failed\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[37mName: \u001b[0m\u001b[31mcrew\u001b[0m                                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m  \u001b[37mID: \u001b[0m\u001b[31m64849065-21cd-4849-9374-b0dab0b883cb\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AuthenticationError",
     "evalue": "litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/llms/openai/openai.py:711\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    710\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/llms/openai/openai.py:614\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    612\u001b[39m         status_code=\u001b[32m422\u001b[39m, message=\u001b[33m\"\u001b[39m\u001b[33mmax retries must be an int\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    613\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m openai_client: OpenAI = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_openai_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_async\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/llms/openai/openai.py:368\u001b[39m, in \u001b[36mOpenAIChatCompletion._get_openai_client\u001b[39m\u001b[34m(self, is_async, api_key, api_base, api_version, timeout, max_retries, organization, client)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     _new_client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m## SAVE CACHE KEY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/openai/_client.py:116\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/main.py:1692\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[39m\n\u001b[32m   1686\u001b[39m     logging.post_call(\n\u001b[32m   1687\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1688\u001b[39m         api_key=api_key,\n\u001b[32m   1689\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1690\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1691\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1692\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1694\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1695\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/main.py:1665\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[39m\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1665\u001b[39m     response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1679\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   1681\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/llms/openai/openai.py:721\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    720\u001b[39m     error_headers = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    722\u001b[39m     status_code=status_code, message=error_text, headers=error_headers\n\u001b[32m    723\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1194\u001b[39m\n\u001b[32m   1191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1162\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1159\u001b[39m nest_asyncio.apply()\n\u001b[32m   1161\u001b[39m \u001b[38;5;66;03m# Now we can use asyncio.run inside Jupyter\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m survey_dict = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkickoff_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msurvey_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msurvey_to_process\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m# Convert the survey to Qualtrics format\u001b[39;00m\n\u001b[32m   1167\u001b[39m qualtrics_payload = survey_dict_to_qualtrics_payload(survey_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1056\u001b[39m, in \u001b[36mSurveyFlow.kickoff_async\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   1047\u001b[39m survey_crew = Crew(\n\u001b[32m   1048\u001b[39m     agents=[convert_agent, editor_agent],\n\u001b[32m   1049\u001b[39m     tasks=[convert_task, research_task, comment_task, improve_task],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1052\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1053\u001b[39m )\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# Run the crew to process the survey\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m crew_result = \u001b[43msurvey_crew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_inputs\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# Parse the result\u001b[39;00m\n\u001b[32m   1061\u001b[39m raw = crew_result.raw.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/crew.py:646\u001b[39m, in \u001b[36mCrew.kickoff\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    643\u001b[39m metrics: List[UsageMetrics] = []\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.sequential:\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.hierarchical:\n\u001b[32m    648\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_hierarchical_process()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/crew.py:758\u001b[39m, in \u001b[36mCrew._run_sequential_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CrewOutput:\n\u001b[32m    757\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/crew.py:861\u001b[39m, in \u001b[36mCrew._execute_tasks\u001b[39m\u001b[34m(self, tasks, start_index, was_replayed)\u001b[39m\n\u001b[32m    858\u001b[39m     futures.clear()\n\u001b[32m    860\u001b[39m context = \u001b[38;5;28mself\u001b[39m._get_context(task, task_outputs)\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m task_output = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mBaseTool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools_for_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m task_outputs.append(task_output)\n\u001b[32m    867\u001b[39m \u001b[38;5;28mself\u001b[39m._process_task_result(task, task_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/task.py:328\u001b[39m, in \u001b[36mTask.execute_sync\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_sync\u001b[39m(\n\u001b[32m    322\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    323\u001b[39m     agent: Optional[BaseAgent] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    324\u001b[39m     context: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    325\u001b[39m     tools: Optional[List[BaseTool]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    326\u001b[39m ) -> TaskOutput:\n\u001b[32m    327\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/task.py:472\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28mself\u001b[39m.end_time = datetime.datetime.now()\n\u001b[32m    471\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error=\u001b[38;5;28mstr\u001b[39m(e), task=\u001b[38;5;28mself\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/task.py:392\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28mself\u001b[39m.processed_by_agents.add(agent.role)\n\u001b[32m    391\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context=context, task=\u001b[38;5;28mself\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m pydantic_output, json_output = \u001b[38;5;28mself\u001b[39m._export_output(result)\n\u001b[32m    399\u001b[39m task_output = TaskOutput(\n\u001b[32m    400\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    401\u001b[39m     description=\u001b[38;5;28mself\u001b[39m.description,\n\u001b[32m   (...)\u001b[39m\u001b[32m    407\u001b[39m     output_format=\u001b[38;5;28mself\u001b[39m._get_output_format(),\n\u001b[32m    408\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/agent.py:269\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m    261\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    262\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    263\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         ),\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    270\u001b[39m \u001b[38;5;28mself\u001b[39m._times_executed += \u001b[32m1\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._times_executed > \u001b[38;5;28mself\u001b[39m.max_retry_limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/agent.py:250\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    241\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    242\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m         event=AgentExecutionStartedEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m         ),\n\u001b[32m    249\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mask_for_human_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    260\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:123\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    120\u001b[39m handle_unknown_error(\u001b[38;5;28mself\u001b[39m._printer, e)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:112\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.ask_for_human_input = \u001b[38;5;28mbool\u001b[39m(inputs.get(\u001b[33m\"\u001b[39m\u001b[33mask_for_human_input\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     formatted_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    115\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    116\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    117\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:208\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_context_length_exceeded(e):\n\u001b[32m    210\u001b[39m         handle_context_length(\n\u001b[32m    211\u001b[39m             respect_context_window=\u001b[38;5;28mself\u001b[39m.respect_context_window,\n\u001b[32m    212\u001b[39m             printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m             i18n=\u001b[38;5;28mself\u001b[39m._i18n,\n\u001b[32m    217\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:155\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    144\u001b[39m     formatted_answer = handle_max_iterations_exceeded(\n\u001b[32m    145\u001b[39m         formatted_answer,\n\u001b[32m    146\u001b[39m         printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m         callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m    151\u001b[39m     )\n\u001b[32m    153\u001b[39m enforce_rpm_limit(\u001b[38;5;28mself\u001b[39m.request_within_rpm_limit)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m answer = \u001b[43mget_llm_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m formatted_answer = process_llm_response(answer, \u001b[38;5;28mself\u001b[39m.use_stop_words)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Extract agent fingerprint if available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:157\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    153\u001b[39m     printer.print(\n\u001b[32m    154\u001b[39m         content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    155\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[32m    159\u001b[39m     printer.print(\n\u001b[32m    160\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mReceived None or empty response from LLM call.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    161\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    162\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:148\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     answer = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    153\u001b[39m     printer.print(\n\u001b[32m    154\u001b[39m         content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    155\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    156\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/llm.py:794\u001b[39m, in \u001b[36mLLM.call\u001b[39m\u001b[34m(self, messages, tools, callbacks, available_functions)\u001b[39m\n\u001b[32m    790\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_streaming_response(\n\u001b[32m    791\u001b[39m             params, callbacks, available_functions\n\u001b[32m    792\u001b[39m         )\n\u001b[32m    793\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_non_streaming_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_functions\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    799\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    800\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    801\u001b[39m         event=LLMCallFailedEvent(error=\u001b[38;5;28mstr\u001b[39m(e)),\n\u001b[32m    802\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/crewai/llm.py:630\u001b[39m, in \u001b[36mLLM._handle_non_streaming_response\u001b[39m\u001b[34m(self, params, callbacks, available_functions)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Handle a non-streaming response from the LLM.\u001b[39;00m\n\u001b[32m    620\u001b[39m \n\u001b[32m    621\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    627\u001b[39m \u001b[33;03m    str: The response text\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# --- 1) Make the completion call\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# --- 2) Extract response message and content\u001b[39;00m\n\u001b[32m    633\u001b[39m response_message = cast(Choices, cast(ModelResponse, response).choices)[\n\u001b[32m    634\u001b[39m     \u001b[32m0\u001b[39m\n\u001b[32m    635\u001b[39m ].message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/utils.py:1154\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1151\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1152\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1153\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/utils.py:1032\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/main.py:3068\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3066\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3067\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2201\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2200\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2201\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2203\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/crewai_agent/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:357\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    353\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    354\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    355\u001b[39m ):\n\u001b[32m    356\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\n\u001b[32m    358\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAuthenticationError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    359\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    360\u001b[39m         model=model,\n\u001b[32m    361\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    362\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    363\u001b[39m     )\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMistral API raised a streaming error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str:\n\u001b[32m    365\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mAuthenticationError\u001b[39m: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import Literal, Dict, List, Any, Union, Optional\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai.tasks.task_output import OutputFormat\n",
    "from crewai.knowledge.source.crew_docling_source import CrewDoclingSource\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "os.environ[\"OTEL_TRACES_EXPORTER\"] = \"none\"\n",
    "\n",
    "# ========== Pydantic Models ==========\n",
    "class ChoiceOption(BaseModel):\n",
    "    text: str\n",
    "    value: str\n",
    "\n",
    "class ChoiceConfig(BaseModel):\n",
    "    options: List[ChoiceOption]\n",
    "\n",
    "class SliderConfig(BaseModel):\n",
    "    min: float\n",
    "    max: float\n",
    "    step: float\n",
    "\n",
    "class TextInputConfig(BaseModel):\n",
    "    placeholder: Optional[str] = None\n",
    "    multiline: bool = False\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question_id: str\n",
    "    question_text: str\n",
    "    input_type: Literal[\"multiple_choice\", \"single_choice\", \"slider\", \"text_input\"]\n",
    "    input_config: Union[ChoiceConfig, SliderConfig, TextInputConfig]\n",
    "\n",
    "class Survey(BaseModel):\n",
    "    theme: str\n",
    "    purpose: str\n",
    "    questions: List[Question]\n",
    "\n",
    "class QuestionComment(BaseModel):\n",
    "    question_id: str\n",
    "    comment: str\n",
    "\n",
    "class AnnotatedSurvey(BaseModel):\n",
    "    survey: Survey\n",
    "    question_comments: List[QuestionComment]\n",
    "    overall_comment: Optional[str]\n",
    "\n",
    "class SurveyImprovementResult(BaseModel):\n",
    "    original_with_comments: AnnotatedSurvey\n",
    "    revised_survey: Survey\n",
    "\n",
    "# ========== Utility to load YAML ==========\n",
    "def load_yaml(path: str) -> dict:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# ========== Agent Definitions ==========\n",
    "def load_agents():\n",
    "    conv_cfg = load_yaml(\"config/agents/survey_convert_agent.yaml\")[\"survey_convert_agent\"]\n",
    "    convert_agent = Agent(\n",
    "        name=\"survey_convert_agent\",\n",
    "        role=conv_cfg[\"role\"],\n",
    "        goal=conv_cfg[\"goal\"],\n",
    "        backstory=conv_cfg[\"backstory\"],\n",
    "        verbose=conv_cfg[\"verbose\"],\n",
    "        allow_delegation=conv_cfg[\"allow_delegation\"]\n",
    "    )\n",
    "\n",
    "    edit_cfg = load_yaml(\"config/agents/survey_editor.yaml\")[\"survey_editor\"]\n",
    "    editor_agent = Agent(\n",
    "        name=\"survey_editor_agent\",\n",
    "        role=edit_cfg[\"role\"],\n",
    "        goal=edit_cfg[\"goal\"],\n",
    "        backstory=edit_cfg[\"backstory\"],\n",
    "        verbose=edit_cfg[\"verbose\"],\n",
    "        allow_delegation=edit_cfg[\"allow_delegation\"]\n",
    "    )\n",
    "    \n",
    "    return convert_agent, editor_agent\n",
    "\n",
    "# ========== Task Definitions ==========\n",
    "def load_tasks(convert_agent, editor_agent):\n",
    "    # Load convert task\n",
    "    conv_t = load_yaml(\"config/tasks/convert_survey_to_json.yaml\")[\"convert_survey_to_json\"]\n",
    "    convert_task = Task(\n",
    "        name=\"convert_survey_to_json\",\n",
    "        description=conv_t[\"description\"],\n",
    "        agent=convert_agent,\n",
    "        tool=conv_t.get(\"tool\"),\n",
    "        expected_output=conv_t[\"expected_output\"],\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "\n",
    "    # Load research task and manually replace placeholders\n",
    "    res_t = load_yaml(\"config/tasks/apply_survey_enhancements.yaml\")[\"research_task\"]\n",
    "    # Remove placeholders by replacing them with actual values or generic text\n",
    "    description = res_t[\"description\"].replace(\"{topic}\", \"the survey topic\").replace(\"{current_year}\", str(datetime.now().year))\n",
    "    expected_output = res_t[\"expected_output\"].replace(\"{topic}\", \"the survey topic\")\n",
    "    \n",
    "    research_task = Task(\n",
    "        name=\"research_task\",\n",
    "        description=description,\n",
    "        agent=convert_agent,\n",
    "        expected_output=expected_output,\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "\n",
    "    # Load comment task\n",
    "    com_t = load_yaml(\"config/tasks/comment_survey_task.yaml\")[\"comment_survey\"]\n",
    "    \n",
    "    comment_task = Task(\n",
    "        name=\"comment_survey\",\n",
    "        description=com_t[\"description\"],\n",
    "        agent=editor_agent,\n",
    "        expected_output=com_t[\"expected_output\"],\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "\n",
    "    # Load improve task\n",
    "    imp_t = load_yaml(\"config/tasks/apply_survey_enhancements.yaml\")[\"improve_survey\"]\n",
    "    # Handle the JSON schema example carefully\n",
    "    description = imp_t[\"description\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    expected_output = imp_t[\"expected_output\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    \n",
    "    improve_task = Task(\n",
    "        name=\"improve_survey\",\n",
    "        description=description,\n",
    "        agent=editor_agent,\n",
    "        expected_output=expected_output,\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "    \n",
    "    return convert_task, research_task, comment_task, improve_task\n",
    "\n",
    "def survey_dict_to_qualtrics_payload(survey_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a custom survey dict to a Qualtrics v3 API survey-definitions payload\n",
    "    Supports question types: multiple_choice, single_choice, slider, text_input\n",
    "    \"\"\"\n",
    "    survey_meta = survey_dict[\"revised_survey\"]\n",
    "    payload = {\n",
    "        \"SurveyName\":      survey_meta.get(\"theme\", \"New Survey\"),\n",
    "        \"Language\":        \"EN\",\n",
    "        \"ProjectCategory\": \"CORE\",\n",
    "        \"Questions\":       {}\n",
    "    }\n",
    "\n",
    "    for q in survey_meta[\"questions\"]:\n",
    "        raw_id = q[\"question_id\"]                  # e.g. \"q4\"\n",
    "        num    = re.sub(r'\\D+', '', raw_id)        # extract number \"4\"\n",
    "        qid    = f\"QID{num}\"                       # assemble \"QID4\"\n",
    "\n",
    "        qt  = q[\"question_text\"]\n",
    "        it  = q[\"input_type\"]\n",
    "        cfg = q[\"input_config\"]\n",
    "\n",
    "        # ---- Common fields ----\n",
    "        qobj = {\n",
    "            \"QuestionText\":      qt,\n",
    "            \"DataExportTag\":     qid,\n",
    "            \"Configuration\":     {\"QuestionDescriptionOption\": \"UseText\"},\n",
    "            \"Validation\":        {\"Settings\": {\"ForceResponse\": \"OFF\", \"Type\": \"None\"}}\n",
    "        }\n",
    "\n",
    "        # ---- Multiple/Single choice questions ----\n",
    "        if it in (\"multiple_choice\", \"single_choice\"):\n",
    "            choices = {}\n",
    "            for opt in cfg.get(\"options\", []):\n",
    "                if \"=\" in opt:\n",
    "                    idx, txt = opt.split(\"=\", 1)\n",
    "                    idx, txt = idx.strip(), txt.strip()\n",
    "                else:\n",
    "                    idx = str(len(choices) + 1)\n",
    "                    txt = opt.strip()\n",
    "                choices[idx] = {\"Display\": txt}\n",
    "\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"MC\",\n",
    "                \"Selector\":     \"SAVR\" if it == \"multiple_choice\" else \"SINGLE\",\n",
    "                \"SubSelector\":  \"TX\",\n",
    "                \"Choices\":      choices\n",
    "            })\n",
    "\n",
    "        # ---- Slider questions ----\n",
    "        elif it == \"slider\":\n",
    "            # Safely read slider parameters (default 0-100, step 1)\n",
    "            start = cfg.get(\"min\", cfg.get(\"start\", 0))\n",
    "            end   = cfg.get(\"max\", cfg.get(\"end\", 100))\n",
    "            step  = cfg.get(\"step\", cfg.get(\"stepSize\", 1))\n",
    "        \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"SL\",\n",
    "                \"Selector\":     \"Slider\",\n",
    "                \"SubSelector\":  \"SL\",\n",
    "                \"SliderStart\":  start,\n",
    "                \"SliderEnd\":    end,\n",
    "                \"SliderStep\":   step\n",
    "            })\n",
    "        # ---- Text input questions ----\n",
    "        elif it == \"text_input\":   \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"TE\",\n",
    "                \"Selector\":     \"ML\"   # Text Entry must use ML\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input_type: {it!r}\")\n",
    "\n",
    "        # Insert into final payload\n",
    "        payload[\"Questions\"][qid] = qobj\n",
    "\n",
    "    return payload\n",
    "\n",
    "class SurveyConversionOutput(BaseModel):\n",
    "    \"\"\"Model to validate the output of the survey conversion agent\"\"\"\n",
    "    title: str\n",
    "    fields: List[Dict[str, Any]]\n",
    "\n",
    "def validate_conversion_output(raw_output: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Validates the output of the survey conversion agent using Pydantic.\n",
    "    \n",
    "    Args:\n",
    "        raw_output: The raw JSON string output from the agent\n",
    "        \n",
    "    Returns:\n",
    "        The validated dictionary if successful\n",
    "        \n",
    "    Raises:\n",
    "        ValidationError: If the output doesn't match the expected schema\n",
    "        ValueError: If the output cannot be parsed as JSON\n",
    "    \"\"\"\n",
    "    # Clean up the raw output - strip markdown code blocks if present\n",
    "    cleaned_output = raw_output.strip()\n",
    "    if cleaned_output.startswith(\"```json\"):\n",
    "        cleaned_output = cleaned_output.split(\"```json\", 1)[1]\n",
    "    if cleaned_output.startswith(\"```\"):\n",
    "        cleaned_output = cleaned_output.split(\"```\", 1)[1]\n",
    "    if \"```\" in cleaned_output:\n",
    "        cleaned_output = cleaned_output.rsplit(\"```\", 1)[0]\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON\n",
    "        parsed_dict = json.loads(cleaned_output)\n",
    "        \n",
    "        # Validate using Pydantic model\n",
    "        validated = SurveyConversionOutput(**parsed_dict)\n",
    "        \n",
    "        # Return the validated dict\n",
    "        return validated.dict()\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Failed to parse JSON output: {e}\\nRaw output:\\n{raw_output}\")\n",
    "    except ValidationError as e:\n",
    "        raise ValidationError(f\"Output validation failed: {e}\\nRaw output:\\n{raw_output}\", SurveyConversionOutput)\n",
    "\n",
    "def convert_to_question_format(conversion_output: Dict) -> List[Question]:\n",
    "    \"\"\"\n",
    "    Converts the validated conversion output to a list of Question objects.\n",
    "    \n",
    "    Args:\n",
    "        conversion_output: The validated conversion output\n",
    "        \n",
    "    Returns:\n",
    "        List of Question objects\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for i, field in enumerate(conversion_output[\"fields\"]):\n",
    "        question_id = f\"q{i+1}\"\n",
    "        question_text = field[\"title\"]\n",
    "        \n",
    "        # Determine input type and config\n",
    "        if field[\"type\"] == \"multiple_choice\":\n",
    "            input_type = \"multiple_choice\"\n",
    "            options = [{\"text\": opt, \"value\": str(i)} for i, opt in enumerate(field.get(\"options\", []))]\n",
    "            input_config = ChoiceConfig(options=options)\n",
    "        elif field[\"type\"] == \"text_input\":\n",
    "            input_type = \"text_input\"\n",
    "            input_config = TextInputConfig(multiline=False)\n",
    "        elif field[\"type\"] == \"slider\":\n",
    "            input_type = \"slider\"\n",
    "            input_config = SliderConfig(min=0, max=100, step=1)  # Default values\n",
    "        else:\n",
    "            # Default to single_choice for most survey questions with scales\n",
    "            input_type = \"single_choice\"\n",
    "            options = [{\"text\": opt, \"value\": str(i)} for i, opt in enumerate(field.get(\"options\", []))]\n",
    "            input_config = ChoiceConfig(options=options)\n",
    "        \n",
    "        # Create Question object\n",
    "        question = Question(\n",
    "            question_id=question_id,\n",
    "            question_text=question_text,\n",
    "            input_type=input_type,\n",
    "            input_config=input_config\n",
    "        )\n",
    "        \n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Update the convert_task to include validation\n",
    "def modified_convert_task(conv_t, convert_agent):\n",
    "    \"\"\"Creates a modified convert task with validation\"\"\"\n",
    "    convert_task = Task(\n",
    "        name=\"convert_survey_to_json\",\n",
    "        description=conv_t[\"description\"],\n",
    "        agent=convert_agent,\n",
    "        tool=conv_t.get(\"tool\"),\n",
    "        expected_output=conv_t[\"expected_output\"],\n",
    "        output_format=OutputFormat.JSON,\n",
    "        async_execution=True,  # Enable async for better performance\n",
    "        validation_function=validate_conversion_output  # Add validation function\n",
    "    )\n",
    "    return convert_task\n",
    "\n",
    "class QualtricsClient:\n",
    "    \"\"\"Handles all Qualtrics API interactions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Qualtrics API client with credentials from .env file\"\"\"\n",
    "        # Print current working directory to help debug file path issues\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        \n",
    "        # Check if .env file exists\n",
    "        if os.path.exists('.env'):\n",
    "            print(\"Found .env file in current directory\")\n",
    "        else:\n",
    "            print(\"WARNING: No .env file found in current directory!\")\n",
    "            \n",
    "        # Load environment variables\n",
    "        load_dotenv(verbose=True)\n",
    "        \n",
    "        self.api_token = os.getenv('QUALTRICS_API_TOKEN')\n",
    "        self.data_center = os.getenv('QUALTRICS_DATA_CENTER')\n",
    "        self.directory_id = os.getenv('QUALTRICS_DIRECTORY_ID')\n",
    "        \n",
    "        # Print obfuscated token for debugging (only first/last 4 chars)\n",
    "        if self.api_token:\n",
    "            token_length = len(self.api_token)\n",
    "            masked_token = self.api_token[:4] + '*' * (token_length - 8) + self.api_token[-4:] if token_length > 8 else \"****\"\n",
    "            print(f\"API Token loaded (masked): {masked_token}\")\n",
    "        else:\n",
    "            print(\"WARNING: No API token found in environment variables!\")\n",
    "            \n",
    "        if self.data_center:\n",
    "            print(f\"Data center: {self.data_center}\")\n",
    "        else:\n",
    "            print(\"WARNING: No data center found in environment variables!\")\n",
    "        \n",
    "        if not self.api_token or not self.data_center:\n",
    "            raise ValueError(\"Missing Qualtrics API credentials in .env file\")\n",
    "            \n",
    "        # Set up base URL for API requests\n",
    "        self.base_url = f\"https://{self.data_center}.qualtrics.com/API/v3/\"\n",
    "        self.headers = {\n",
    "            \"X-API-Token\": self.api_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Test connection\n",
    "        print(\"Testing Qualtrics API connection...\")\n",
    "        try:\n",
    "            test_url = f\"{self.base_url}whoami\"\n",
    "            response = requests.get(test_url, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                user_info = response.json()[\"result\"]\n",
    "                print(f\"Connection successful! Authenticated as: {user_info.get('firstName', '')} {user_info.get('lastName', '')}\")\n",
    "            else:\n",
    "                print(f\"Connection test failed with status code: {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing connection: {str(e)}\")\n",
    "        \n",
    "    def create_survey(self, survey_name, survey_template=None):\n",
    "        \"\"\"\n",
    "        Create a new survey in Qualtrics\n",
    "        \n",
    "        Args:\n",
    "            survey_name (str): Name of the survey\n",
    "            survey_template (dict, optional): Survey template JSON\n",
    "            \n",
    "        Returns:\n",
    "            str: Survey ID of the created survey\n",
    "        \"\"\"\n",
    "        print(f\"Creating survey: {survey_name}\")\n",
    "        \n",
    "        # If no template is provided, use a basic template\n",
    "        if not survey_template:\n",
    "            # Define the survey payload with required fields including ProjectCategory\n",
    "            survey_payload = {\n",
    "                \"SurveyName\": survey_name,\n",
    "                \"Language\": \"EN\",\n",
    "                \"ProjectCategory\": \"CORE\", # Required field\n",
    "                \"Questions\": {\n",
    "                    \"QID1\": {\n",
    "                        \"QuestionText\": \"What is your age?\",\n",
    "                        \"QuestionType\": \"MC\",\n",
    "                        \"Selector\": \"SAVR\", # Required selector for multiple choice\n",
    "                        \"SubSelector\": \"TX\", # Text selector\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"Choices\": {\n",
    "                            \"1\": {\"Display\": \"18-24\"},\n",
    "                            \"2\": {\"Display\": \"25-34\"},\n",
    "                            \"3\": {\"Display\": \"35-44\"},\n",
    "                            \"4\": {\"Display\": \"45-54\"},\n",
    "                            \"5\": {\"Display\": \"55-64\"},\n",
    "                            \"6\": {\"Display\": \"65+\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"QID2\": {\n",
    "                        \"QuestionText\": \"How satisfied are you with our product?\",\n",
    "                        \"QuestionType\": \"Likert\",\n",
    "                        \"Selector\": \"LSL\", # Likert scale\n",
    "                        \"SubSelector\": \"TX\", # Text selector\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"Choices\": {\n",
    "                            \"1\": {\"Display\": \"Very dissatisfied\"},\n",
    "                            \"2\": {\"Display\": \"Dissatisfied\"},\n",
    "                            \"3\": {\"Display\": \"Neutral\"},\n",
    "                            \"4\": {\"Display\": \"Satisfied\"},\n",
    "                            \"5\": {\"Display\": \"Very satisfied\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"QID3\": {\n",
    "                        \"QuestionText\": \"Any additional comments?\",\n",
    "                        \"QuestionType\": \"TE\", # Text entry\n",
    "                        \"Selector\": \"ML\", # Multi-line\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # If a template is provided, make sure it includes ProjectCategory\n",
    "            survey_payload = survey_template\n",
    "            if \"ProjectCategory\" not in survey_payload:\n",
    "                survey_payload[\"ProjectCategory\"] = \"CORE\"\n",
    "        \n",
    "        # Create survey\n",
    "        url = f\"{self.base_url}survey-definitions\"\n",
    "        payload = json.dumps(survey_payload)\n",
    "        \n",
    "        print(f\"Sending payload to Qualtrics: {payload[:200]}...\")\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, data=payload)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error response: {response.text}\")\n",
    "            raise Exception(f\"Failed to create survey: {response.text}\")\n",
    "        \n",
    "        result = response.json()\n",
    "        survey_id = result[\"result\"][\"SurveyID\"]\n",
    "        print(f\"Survey created successfully with ID: {survey_id}\")\n",
    "        \n",
    "        return survey_id\n",
    "\n",
    "    def add_questions(self, survey_id: str, questions: List[dict]):\n",
    "        \"\"\"Add questions to a survey\"\"\"\n",
    "        for q in questions:\n",
    "            # Start with the fields every question needs\n",
    "            q_payload = {\n",
    "                 \"QuestionID\":   q[\"question_id\"],\n",
    "                 \"QuestionText\": q[\"question_text\"],\n",
    "                 \"QuestionType\": q[\"QuestionType\"],\n",
    "                 \"DataExportTag\": q[\"question_id\"],\n",
    "                 \"Configuration\": {\"QuestionDescriptionOption\": \"UseText\"},\n",
    "                 \"Validation\":    {\"Settings\": {\"ForceResponse\": \"OFF\", \"Type\": \"None\"}},\n",
    "             }\n",
    "\n",
    "            # Only add Selector/SubSelector if given\n",
    "            if \"Selector\" in q:\n",
    "                q_payload[\"Selector\"] = q[\"Selector\"]\n",
    "            if \"SubSelector\" in q:\n",
    "                q_payload[\"SubSelector\"] = q[\"SubSelector\"]\n",
    "            # Only add Choices if given\n",
    "            if \"Choices\" in q:\n",
    "                q_payload[\"Choices\"] = q[\"Choices\"]\n",
    "    \n",
    "            url = f\"{self.base_url}survey-definitions/{survey_id}/questions\"\n",
    "            resp = requests.post(url, headers=self.headers, json=q_payload)\n",
    "            print(f\"POST questions → {resp.status_code}\", resp.json())\n",
    "\n",
    "    def add_block(self, survey_id: str, block_payload: dict):\n",
    "        \"\"\"Add a block to a survey\"\"\"\n",
    "        url = f\"{self.base_url}survey-definitions/{survey_id}/blocks\"\n",
    "        resp = requests.post(url, headers=self.headers, json=block_payload)\n",
    "        print(f\"POST blocks → {resp.status_code}\", resp.json())\n",
    "\n",
    "    def update_flow(self, survey_id: str, flow_payload: dict):\n",
    "        \"\"\"Update the flow of a survey\"\"\"\n",
    "        url = f\"{self.base_url}survey-definitions/{survey_id}/flow\"\n",
    "        resp = requests.put(url, headers=self.headers, json=flow_payload)\n",
    "        print(\"PUT flow →\", resp.status_code, resp.json())\n",
    "    \n",
    "    def activate_survey(self, survey_id):\n",
    "        \"\"\"\n",
    "        Activate a survey to make it available for distribution\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey to activate\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful\n",
    "        \"\"\"\n",
    "        print(f\"Activating survey: {survey_id}\")\n",
    "        \n",
    "        url = f\"{self.base_url}surveys/{survey_id}\"\n",
    "        payload = json.dumps({\"isActive\": True})\n",
    "        \n",
    "        response = requests.put(url, headers=self.headers, data=payload)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to activate survey: {response.text}\")\n",
    "        \n",
    "        print(f\"Survey activated successfully\")\n",
    "        return True\n",
    "    \n",
    "    def create_distribution_link(self, survey_id, link_type=\"Anonymous\"):\n",
    "        \"\"\"\n",
    "        Create a distribution link for a survey\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey to distribute\n",
    "            link_type (str): Type of link (Anonymous or Individual)\n",
    "            \n",
    "        Returns:\n",
    "            str: Distribution link URL\n",
    "        \"\"\"\n",
    "        print(f\"Creating distribution link for survey: {survey_id}\")\n",
    "        \n",
    "        # For anonymous links, we can construct the URL directly based on the standard pattern\n",
    "        # https://DATACENTERID.qualtrics.com/jfe/form/SURVEYID\n",
    "        if link_type == \"Anonymous\":\n",
    "            survey_link = f\"https://{self.data_center}.qualtrics.com/jfe/form/{survey_id}\"\n",
    "            print(f\"Anonymous survey link created: {survey_link}\")\n",
    "            return survey_link\n",
    "        \n",
    "        # For other distribution types, we would use the API, but that's not implemented yet\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Distribution type '{link_type}' is not yet supported\")\n",
    "    \n",
    "    def get_survey_responses(self, survey_id, file_format=\"csv\"):\n",
    "        \"\"\"\n",
    "        Download survey responses\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey\n",
    "            file_format (str): Format of the response file (csv, json, spss, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Survey responses as a DataFrame\n",
    "        \"\"\"\n",
    "        print(f\"Downloading responses for survey: {survey_id}\")\n",
    "        \n",
    "        # Step 1: Create the export\n",
    "        export_url = f\"{self.base_url}surveys/{survey_id}/export-responses\"\n",
    "        export_payload = json.dumps({\n",
    "            \"format\": file_format,\n",
    "            \"useLabels\": True\n",
    "        })\n",
    "        \n",
    "        export_response = requests.post(export_url, headers=self.headers, data=export_payload)\n",
    "        \n",
    "        if export_response.status_code != 200:\n",
    "            raise Exception(f\"Failed to initiate export: {export_response.text}\")\n",
    "        \n",
    "        progress_id = export_response.json()[\"result\"][\"progressId\"]\n",
    "        \n",
    "        # Step 2: Check export progress\n",
    "        progress_status = \"inProgress\"\n",
    "        progress = 0\n",
    "        \n",
    "        while progress_status != \"complete\" and progress < 100:\n",
    "            progress_url = f\"{self.base_url}surveys/{survey_id}/export-responses/{progress_id}\"\n",
    "            progress_response = requests.get(progress_url, headers=self.headers)\n",
    "            \n",
    "            if progress_response.status_code != 200:\n",
    "                raise Exception(f\"Failed to check export progress: {progress_response.text}\")\n",
    "            \n",
    "            progress_result = progress_response.json()[\"result\"]\n",
    "            progress_status = progress_result[\"status\"]\n",
    "            progress = progress_result.get(\"percentComplete\", 0)\n",
    "            \n",
    "            print(f\"Export progress: {progress}%\")\n",
    "            \n",
    "            if progress_status != \"complete\" and progress < 100:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Step 3: Download the file\n",
    "        file_id = progress_result[\"fileId\"]\n",
    "        download_url = f\"{self.base_url}surveys/{survey_id}/export-responses/{file_id}/file\"\n",
    "        download_response = requests.get(download_url, headers=self.headers)\n",
    "        \n",
    "        if download_response.status_code != 200:\n",
    "            raise Exception(f\"Failed to download responses: {download_response.text}\")\n",
    "        \n",
    "        # Step 4: Extract and parse the zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(download_response.content)) as zip_file:\n",
    "            data_file = [f for f in zip_file.namelist() if f.endswith(f\".{file_format}\")][0]\n",
    "            with zip_file.open(data_file) as file:\n",
    "                if file_format == \"csv\":\n",
    "                    df = pd.read_csv(file)\n",
    "                elif file_format == \"json\":\n",
    "                    df = pd.read_json(file)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "        \n",
    "        print(f\"Successfully downloaded {len(df)} responses\")\n",
    "        return df\n",
    "\n",
    "class MTurkClient:\n",
    "    \"\"\"Handles all MTurk API interactions\"\"\"\n",
    "    def __init__(self, \n",
    "                aws_access_key_id: str = None, \n",
    "                aws_secret_access_key: str = None, \n",
    "                use_sandbox: bool = True):  # Default to sandbox mode for safety\n",
    "        \"\"\"\n",
    "        Initialize MTurk client\n",
    "        \n",
    "        Args:\n",
    "            aws_access_key_id: Optional override for AWS access key\n",
    "            aws_secret_access_key: Optional override for AWS secret key\n",
    "            use_sandbox: Boolean for using sandbox (defaults to True for safety)\n",
    "        \"\"\"\n",
    "        # Load from .env file\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Set AWS credentials (with optional overrides)\n",
    "        self.aws_access_key_id = aws_access_key_id or os.getenv('AWS_ACCESS_KEY_ID')\n",
    "        self.aws_secret_access_key = aws_secret_access_key or os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "        \n",
    "        # Check if credentials are available\n",
    "        if not self.aws_access_key_id or not self.aws_secret_access_key:\n",
    "            raise ValueError(\"Missing AWS credentials in .env file or constructor parameters\")\n",
    "\n",
    "        # Determine sandbox mode (with optional override)\n",
    "        if use_sandbox is None:\n",
    "            # Read from environment if not provided in constructor\n",
    "            self.use_sandbox = os.getenv('MTURK_SANDBOX', 'True').lower() == 'true'\n",
    "        else:\n",
    "            self.use_sandbox = use_sandbox\n",
    "\n",
    "        # Set endpoint based on sandbox mode\n",
    "        region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "        endpoint = (\n",
    "            'https://mturk-requester-sandbox.us-east-1.amazonaws.com'\n",
    "            if self.use_sandbox else\n",
    "            'https://mturk-requester.us-east-1.amazonaws.com'\n",
    "        )\n",
    "\n",
    "        # Create boto3 client\n",
    "        try:\n",
    "            self.client = boto3.client(\n",
    "                'mturk',\n",
    "                aws_access_key_id=self.aws_access_key_id,\n",
    "                aws_secret_access_key=self.aws_secret_access_key,\n",
    "                region_name=region,\n",
    "                endpoint_url=endpoint\n",
    "            )\n",
    "            print(f\"MTurk client initialized in {'Sandbox' if self.use_sandbox else 'Production'} mode\")\n",
    "            \n",
    "            # Verify connection by checking account balance\n",
    "            self.get_account_balance()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing MTurk client: {str(e)}\")\n",
    "            print(\"Please verify your AWS credentials and MTurk account configuration.\")\n",
    "            print(\"For MTurk integration, you need:\")\n",
    "            print(\"1. Valid AWS credentials in your .env file\")\n",
    "            print(\"2. Your AWS account linked to your MTurk Requester account\")\n",
    "            print(\"3. Proper permissions for the MTurk API\")\n",
    "            \n",
    "            # Create a dummy client for graceful degradation\n",
    "            self.client = None\n",
    "            self.connection_error = str(e)\n",
    "            \n",
    "    def get_account_balance(self):\n",
    "        \"\"\"Get the available MTurk account balance\"\"\"\n",
    "        if not self.client:\n",
    "            print(f\"Cannot check balance: {self.connection_error}\")\n",
    "            return 0.0\n",
    "            \n",
    "        try:\n",
    "            response = self.client.get_account_balance()\n",
    "            balance = response['AvailableBalance']\n",
    "            print(f\"MTurk account balance: ${balance}\")\n",
    "            return float(balance)\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking balance: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def create_hit_with_survey_link(self, survey_link, hit_config=None):\n",
    "        \"\"\"\n",
    "        Create an MTurk HIT with a link to a Qualtrics survey\n",
    "        \n",
    "        Args:\n",
    "            survey_link (str): URL to the Qualtrics survey\n",
    "            hit_config (dict, optional): Custom configuration for the HIT\n",
    "            \n",
    "        Returns:\n",
    "            str: HIT ID\n",
    "        \"\"\"\n",
    "        print(\"Creating MTurk HIT with survey link\")\n",
    "        \n",
    "        # Default HIT configuration\n",
    "        if not hit_config:\n",
    "            hit_config = {\n",
    "                'Title': 'Complete a short survey',\n",
    "                'Description': 'We need your input for a quick survey that should take less than 10 minutes',\n",
    "                'Keywords': 'survey, research, opinion, feedback',\n",
    "                'Reward': '0.50',\n",
    "                'MaxAssignments': 100,\n",
    "                'LifetimeInSeconds': 86400,  # 1 day\n",
    "                'AssignmentDurationInSeconds': 1800,  # 30 minutes\n",
    "                'AutoApprovalDelayInSeconds': 86400,  # 1 day\n",
    "                'QualificationRequirements': []\n",
    "            }\n",
    "        \n",
    "        # Create the HTML question with the survey link\n",
    "        question_html = f\"\"\"\n",
    "        <HTMLQuestion xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd\">\n",
    "            <HTMLContent><![CDATA[\n",
    "                <!DOCTYPE html>\n",
    "                <html>\n",
    "                <head>\n",
    "                    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
    "                    <script type='text/javascript' src='https://s3.amazonaws.com/mturk-public/externalHIT_v1.js'></script>\n",
    "                </head>\n",
    "                <body>\n",
    "                    <form name='mturk_form' method='post' id='mturk_form' action='https://www.mturk.com/mturk/externalSubmit'>\n",
    "                        <input type='hidden' value='' name='assignmentId' id='assignmentId'/>\n",
    "                        <h1>Survey Task</h1>\n",
    "                        <p>Please complete the survey at the following link:</p>\n",
    "                        <p><a href='{survey_link}' target='_blank'>{survey_link}</a></p>\n",
    "                        <p>After completing the survey, you will receive a completion code. Enter the code below:</p>\n",
    "                        <p><input type='text' name='completion_code' id='completion_code' size='40'/></p>\n",
    "                        <p><input type='submit' id='submitButton' value='Submit' /></p>\n",
    "                    </form>\n",
    "                    <script language='Javascript'>\n",
    "                        turkSetAssignmentID();\n",
    "                    </script>\n",
    "                </body>\n",
    "                </html>\n",
    "            ]]></HTMLContent>\n",
    "            <FrameHeight>600</FrameHeight>\n",
    "        </HTMLQuestion>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the HIT\n",
    "        response = self.client.create_hit(\n",
    "            Title=hit_config['Title'],\n",
    "            Description=hit_config['Description'],\n",
    "            Keywords=hit_config['Keywords'],\n",
    "            Reward=hit_config['Reward'],\n",
    "            MaxAssignments=hit_config['MaxAssignments'],\n",
    "            LifetimeInSeconds=hit_config['LifetimeInSeconds'],\n",
    "            AssignmentDurationInSeconds=hit_config['AssignmentDurationInSeconds'],\n",
    "            AutoApprovalDelayInSeconds=hit_config['AutoApprovalDelayInSeconds'],\n",
    "            Question=question_html,\n",
    "            QualificationRequirements=hit_config['QualificationRequirements']\n",
    "        )\n",
    "        \n",
    "        hit_id = response['HIT']['HITId']\n",
    "        hit_type_id = response['HIT']['HITTypeId']\n",
    "        \n",
    "        print(f\"HIT created successfully with ID: {hit_id}\")\n",
    "        \n",
    "        # Print the HIT URL\n",
    "        if self.use_sandbox:\n",
    "            worker_url = f\"https://workersandbox.mturk.com/mturk/preview?groupId={hit_type_id}\"\n",
    "        else:\n",
    "            worker_url = f\"https://worker.mturk.com/mturk/preview?groupId={hit_type_id}\"\n",
    "            \n",
    "        print(f\"Workers can access the HIT at: {worker_url}\")\n",
    "        \n",
    "        return hit_id\n",
    "    \n",
    "    def get_hit_assignments(self, hit_id):\n",
    "        \"\"\"\n",
    "        Get all assignments for a HIT\n",
    "        \n",
    "        Args:\n",
    "            hit_id (str): ID of the HIT\n",
    "            \n",
    "        Returns:\n",
    "            list: List of assignment dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"Getting assignments for HIT: {hit_id}\")\n",
    "        \n",
    "        # List to store all assignments\n",
    "        all_assignments = []\n",
    "        \n",
    "        # Get assignments with pagination\n",
    "        next_token = None\n",
    "        \n",
    "        while True:\n",
    "            if next_token:\n",
    "                response = self.client.list_assignments_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    NextToken=next_token,\n",
    "                    MaxResults=100\n",
    "                )\n",
    "            else:\n",
    "                response = self.client.list_assignments_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    MaxResults=100\n",
    "                )\n",
    "            \n",
    "            all_assignments.extend(response['Assignments'])\n",
    "            \n",
    "            if 'NextToken' in response:\n",
    "                next_token = response['NextToken']\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(all_assignments)} assignments\")\n",
    "        return all_assignments\n",
    "    \n",
    "    def approve_assignments(self, assignments, feedback=None):\n",
    "        \"\"\"\n",
    "        Approve multiple assignments\n",
    "        \n",
    "        Args:\n",
    "            assignments (list): List of assignment dictionaries or IDs\n",
    "            feedback (str, optional): Feedback to workers\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of successfully approved assignments\n",
    "        \"\"\"\n",
    "        approved_count = 0\n",
    "        \n",
    "        for assignment in assignments:\n",
    "            # Extract assignment ID if a dictionary was provided\n",
    "            assignment_id = assignment['AssignmentId'] if isinstance(assignment, dict) else assignment\n",
    "            \n",
    "            try:\n",
    "                self.client.approve_assignment(\n",
    "                    AssignmentId=assignment_id,\n",
    "                    RequesterFeedback=feedback if feedback else \"Thank you for your participation!\"\n",
    "                )\n",
    "                approved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error approving assignment {assignment_id}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Successfully approved {approved_count} assignments\")\n",
    "        return approved_count\n",
    "    \n",
    "    def delete_hit(self, hit_id):\n",
    "        \"\"\"\n",
    "        Delete a HIT\n",
    "        \n",
    "        Args:\n",
    "            hit_id (str): ID of the HIT to delete\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get the HIT status\n",
    "            hit = self.client.get_hit(HITId=hit_id)\n",
    "            status = hit['HIT']['HITStatus']\n",
    "            \n",
    "            # If the HIT is reviewable, dispose of it\n",
    "            if status == 'Reviewable':\n",
    "                self.client.delete_hit(HITId=hit_id)\n",
    "                print(f\"HIT {hit_id} deleted successfully\")\n",
    "                return True\n",
    "            \n",
    "            # If the HIT is assignable, expire it first then delete it\n",
    "            elif status == 'Assignable':\n",
    "                self.client.update_expiration_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    ExpireAt=datetime(2015, 1, 1)  # Set to a past date to expire immediately\n",
    "                )\n",
    "                time.sleep(1)  # Give time for the HIT to update\n",
    "                self.client.delete_hit(HITId=hit_id)\n",
    "                print(f\"HIT {hit_id} expired and deleted successfully\")\n",
    "                return True\n",
    "                \n",
    "            else:\n",
    "                print(f\"Cannot delete HIT {hit_id}, status is {status}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting HIT {hit_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Add the missing QualtricsAndMTurkAutomation class\n",
    "class QualtricsAndMTurkAutomation:\n",
    "    \"\"\"Handles the automation of creating Qualtrics surveys and MTurk HITs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the automation with Qualtrics and MTurk clients\"\"\"\n",
    "        self.qualtrics_client = QualtricsClient()\n",
    "        self.mturk_client = MTurkClient()\n",
    "    \n",
    "    def run(self, qualtrics_payload, hit_config=None):\n",
    "        \"\"\"\n",
    "        Run the automation to create a Qualtrics survey and MTurk HIT\n",
    "        \n",
    "        Args:\n",
    "            qualtrics_payload (dict): Survey definition for Qualtrics\n",
    "            hit_config (dict, optional): Configuration for MTurk HIT\n",
    "            \n",
    "        Returns:\n",
    "            dict: Results including survey ID, survey link, and HIT ID\n",
    "        \"\"\"\n",
    "        print(\"Starting Qualtrics and MTurk automation...\")\n",
    "        \n",
    "        # Create Qualtrics survey\n",
    "        survey_name = qualtrics_payload.get(\"SurveyName\", \"New Survey\")\n",
    "        survey_id = self.qualtrics_client.create_survey(survey_name, qualtrics_payload)\n",
    "        \n",
    "        # Activate the survey\n",
    "        self.qualtrics_client.activate_survey(survey_id)\n",
    "        \n",
    "        # Get distribution link\n",
    "        survey_link = self.qualtrics_client.create_distribution_link(survey_id)\n",
    "        \n",
    "        # Create MTurk HIT\n",
    "        hit_id = self.mturk_client.create_hit_with_survey_link(survey_link, hit_config)\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"survey_id\": survey_id,\n",
    "            \"survey_link\": survey_link,\n",
    "            \"hit_id\": hit_id\n",
    "        }\n",
    "    \n",
    "    def collect_and_process_results(self, survey_id, hit_id, auto_approve=True):\n",
    "        \"\"\"\n",
    "        Collect and process results from Qualtrics and MTurk\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): Qualtrics survey ID\n",
    "            hit_id (str): MTurk HIT ID\n",
    "            auto_approve (bool): Whether to automatically approve assignments\n",
    "            \n",
    "        Returns:\n",
    "            dict: Results including responses and assignment data\n",
    "        \"\"\"\n",
    "        # Get Qualtrics responses\n",
    "        responses = self.qualtrics_client.get_survey_responses(survey_id)\n",
    "        \n",
    "        # Save responses to CSV\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"survey_responses_{timestamp}.csv\"\n",
    "        responses.to_csv(csv_filename, index=False)\n",
    "        print(f\"Saved {len(responses)} responses to {csv_filename}\")\n",
    "        \n",
    "        # Get MTurk assignments\n",
    "        assignments = self.mturk_client.get_hit_assignments(hit_id)\n",
    "        \n",
    "        # Auto-approve assignments if requested\n",
    "        approved_count = 0\n",
    "        if auto_approve and assignments:\n",
    "            approved_count = self.mturk_client.approve_assignments(assignments)\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"responses\": responses,\n",
    "            \"csv_filename\": csv_filename,\n",
    "            \"assignments\": assignments,\n",
    "            \"approved_count\": approved_count\n",
    "        }\n",
    "\n",
    "# Fix the SurveyFlow class to work with crewAI's updated API\n",
    "from crewai import Flow\n",
    "\n",
    "class SurveyFlow(Flow):\n",
    "    \"\"\"Flow for processing a survey from text to deployment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the flow\"\"\"\n",
    "        super().__init__()\n",
    "        self._custom_state = {}  # Use a custom state dictionary instead\n",
    "        self.verbose = True\n",
    "    \n",
    "    async def kickoff_async(self, inputs):\n",
    "        \"\"\"Process survey and create Qualtrics survey and MTurk HIT\"\"\"\n",
    "        # Store inputs in our custom state\n",
    "        self._custom_state.update(inputs)\n",
    "        \n",
    "        # Get the survey text from state\n",
    "        survey_text = self._custom_state['survey_text'].strip()\n",
    "        first_line = survey_text.splitlines()[0]\n",
    "        topic = first_line.replace('Topic:', '').strip()\n",
    "        current_year = datetime.now().year\n",
    "\n",
    "        # Initialize agents and tasks\n",
    "        convert_agent, editor_agent = load_agents()\n",
    "        convert_task, research_task, comment_task, improve_task = load_tasks(convert_agent, editor_agent)\n",
    "        \n",
    "        # Create task input dictionary\n",
    "        task_inputs = {\n",
    "            'survey_text': survey_text,\n",
    "            'topic': topic,\n",
    "            'current_year': current_year\n",
    "        }\n",
    "        # Include knowledge. Everything in the knowledge file\n",
    "        content_source = CrewDoclingSource(\n",
    "            file_paths=[ \n",
    "                \"diamantopoulos-winklhofer-2001-index-construction-with-formative-indicators-an-alternative-to-scale-development.pdf\",\n",
    "                \"Marketing_survey_research_best_practice_2018.pdf\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Create the crew\n",
    "        survey_crew = Crew(\n",
    "            agents=[convert_agent, editor_agent],\n",
    "            tasks=[convert_task, research_task, comment_task, improve_task],\n",
    "            process=Process.sequential,\n",
    "            knowledge_sources=[content_source],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Run the crew to process the survey\n",
    "        crew_result = survey_crew.kickoff(\n",
    "            inputs=task_inputs\n",
    "        )\n",
    "\n",
    "        # Parse the result\n",
    "        raw = crew_result.raw.strip()\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "    \n",
    "        try:\n",
    "            survey_dict = json.loads(raw)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"JSON parsing failed: {e}\\nRaw output:\\n{raw}\")\n",
    "\n",
    "        # Print the results safely - enhanced with better error handling\n",
    "        print(\"\\n===== Survey Results =====\")\n",
    "        try:\n",
    "            # Try to get original survey with comments\n",
    "            if 'original_with_comments' in survey_dict:\n",
    "                print(\"\\n=== Original Survey (with comments) ===\")\n",
    "                annotated = survey_dict.get('original_with_comments', {})\n",
    "                survey = annotated.get('survey', {})\n",
    "                \n",
    "                # Print theme and purpose safely\n",
    "                print(f\"Theme: {survey.get('theme', 'N/A')}\")\n",
    "                print(f\"Purpose: {survey.get('purpose', 'N/A')}\\n\")\n",
    "                \n",
    "                # Safely iterate through questions\n",
    "                comments = annotated.get('question_comments', [])\n",
    "                for q in survey.get('questions', []):\n",
    "                    if isinstance(q, dict):\n",
    "                        qid = q.get('question_id', 'unknown')\n",
    "                        print(f\"Question {qid}: {q.get('question_text', 'N/A')}\")\n",
    "                        comment = next((c.get('comment', '') for c in comments if c.get('question_id') == qid), None)\n",
    "                        if comment:\n",
    "                            print(f\"  Comment: {comment}\")\n",
    "                        print()\n",
    "                \n",
    "                # Print overall comment if available\n",
    "                overall = annotated.get('overall_comment')\n",
    "                if overall:\n",
    "                    print(f\"Overall comment: {overall}\\n\")\n",
    "            \n",
    "            # Try to get revised survey\n",
    "            if 'revised_survey' in survey_dict:\n",
    "                print(\"=== Revised Survey ===\")\n",
    "                revised = survey_dict.get('revised_survey', {})\n",
    "                \n",
    "                # Print theme and purpose safely\n",
    "                print(f\"Theme:   {revised.get('theme', 'N/A')}\")\n",
    "                print(f\"Purpose: {revised.get('purpose', 'N/A')}\\n\")\n",
    "                \n",
    "                # Safely iterate through questions\n",
    "                for q in revised.get('questions', []):\n",
    "                    if isinstance(q, dict):\n",
    "                        qid = q.get('question_id', 'unknown')\n",
    "                        print(f\"Q{qid}: {q.get('question_text', 'N/A')}\")\n",
    "                        \n",
    "                        # Safely get options\n",
    "                        input_config = q.get('input_config', {})\n",
    "                        if isinstance(input_config, dict):\n",
    "                            opts = input_config.get('options', [])\n",
    "                            if opts:\n",
    "                                print(\"  Options:\")\n",
    "                                for o in opts:\n",
    "                                    print(f\"    - {o}\")\n",
    "                        print()\n",
    "            \n",
    "            # If standard format is not found, print raw structure\n",
    "            if 'original_with_comments' not in survey_dict and 'revised_survey' not in survey_dict:\n",
    "                print(\"Survey output doesn't match expected structure. Raw output:\")\n",
    "                print(json.dumps(survey_dict, indent=2))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing survey structure: {str(e)}\")\n",
    "            print(\"Raw survey data:\")\n",
    "            print(json.dumps(survey_dict, indent=2))\n",
    "            \n",
    "        # Store the survey dict in our custom state\n",
    "        self._custom_state['survey_dict'] = survey_dict\n",
    "        return survey_dict\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the survey processing and deployment flow\"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Display input requirements\n",
    "    print(\"========================================\")\n",
    "    print(\"⚠️  INPUT REQUIREMENTS:\")\n",
    "    print(\"- You must include a line starting with 'Topic:'\")\n",
    "    print(\"- You must include at least one line starting with 'Questions:'\")\n",
    "    print(\"Otherwise, the survey cannot be processed.\")\n",
    "    print(\"========================================\")\n",
    "    \n",
    "    # Get survey input\n",
    "    survey_to_process = input(\"Please enter the Survey content: \")\n",
    "\n",
    "    # Initialize and run the flow - FIXED: completely removed parameters\n",
    "    flow = SurveyFlow()\n",
    "    \n",
    "    # Execute the flow using nest_asyncio to handle Jupyter's event loop\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    # Now we can use asyncio.run inside Jupyter\n",
    "    survey_dict = asyncio.run(flow.kickoff_async(inputs={\n",
    "        'survey_text': survey_to_process\n",
    "    }))\n",
    "\n",
    "    # Convert the survey to Qualtrics format\n",
    "    qualtrics_payload = survey_dict_to_qualtrics_payload(survey_dict)\n",
    "\n",
    "    # Create HIT configuration\n",
    "    hit_config = {\n",
    "        'Title': 'Complete a short survey on organic food',\n",
    "        'Description': survey_dict[\"revised_survey\"][\"purpose\"],\n",
    "        'Keywords': 'survey, research, feedback',\n",
    "        'Reward': '0.75',\n",
    "        'MaxAssignments': 100,\n",
    "        'LifetimeInSeconds': 86400,\n",
    "        'AssignmentDurationInSeconds': 1800,\n",
    "        'AutoApprovalDelayInSeconds': 86400,\n",
    "        'QualificationRequirements': []\n",
    "    }\n",
    "    \n",
    "    # Run the automation to create the survey and HIT\n",
    "    automation = QualtricsAndMTurkAutomation()\n",
    "    results = automation.run(qualtrics_payload, hit_config)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Survey ID: {results['survey_id']}\")\n",
    "    print(f\"Survey Link: {results['survey_link']}\")\n",
    "    print(f\"HIT ID: {results['hit_id']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to collect data for Survey ID: SV_ehDc3d6TiGQiefY and HIT ID: 3FJ2RVH26P35OQIGA3KFR9ISICK296\n",
      "Current working directory: /Users/princess/Documents/RA/Field-Experiment-AI-Agent\n",
      "Found .env file in current directory\n",
      "API Token loaded (masked): yNZ9********************************eAPM\n",
      "Data center: yul1\n",
      "Testing Qualtrics API connection...\n",
      "Connection successful! Authenticated as: Sichen Zhong\n",
      "MTurk client initialized in Sandbox mode\n",
      "MTurk account balance: $10000.00\n",
      "Downloading responses for survey: SV_ehDc3d6TiGQiefY\n",
      "Export progress: 0.0%\n",
      "Export progress: 100.0%\n",
      "Successfully downloaded 2 responses\n",
      "Saved 2 responses to survey_responses_20250513_135954.csv\n",
      "Getting assignments for HIT: 3FJ2RVH26P35OQIGA3KFR9ISICK296\n",
      "Found 0 assignments\n",
      "Data collection completed. Summary:\n",
      "{'responses':                                  StartDate  \\\n",
      "0                               Start Date   \n",
      "1  {\"ImportId\":\"startDate\",\"timeZone\":\"Z\"}   \n",
      "\n",
      "                                 EndDate                 Status  \\\n",
      "0                               End Date          Response Type   \n",
      "1  {\"ImportId\":\"endDate\",\"timeZone\":\"Z\"}  {\"ImportId\":\"status\"}   \n",
      "\n",
      "                  IPAddress                 Progress    Duration (in seconds)  \\\n",
      "0                IP Address                 Progress    Duration (in seconds)   \n",
      "1  {\"ImportId\":\"ipAddress\"}  {\"ImportId\":\"progress\"}  {\"ImportId\":\"duration\"}   \n",
      "\n",
      "                  Finished                                RecordedDate  \\\n",
      "0                 Finished                               Recorded Date   \n",
      "1  {\"ImportId\":\"finished\"}  {\"ImportId\":\"recordedDate\",\"timeZone\":\"Z\"}   \n",
      "\n",
      "                 ResponseId                 RecipientLastName  \\\n",
      "0               Response ID               Recipient Last Name   \n",
      "1  {\"ImportId\":\"_recordId\"}  {\"ImportId\":\"recipientLastName\"}   \n",
      "\n",
      "                  RecipientFirstName                 RecipientEmail  \\\n",
      "0               Recipient First Name                Recipient Email   \n",
      "1  {\"ImportId\":\"recipientFirstName\"}  {\"ImportId\":\"recipientEmail\"}   \n",
      "\n",
      "                      ExternalReference                 LocationLatitude  \\\n",
      "0               External Data Reference                Location Latitude   \n",
      "1  {\"ImportId\":\"externalDataReference\"}  {\"ImportId\":\"locationLatitude\"}   \n",
      "\n",
      "                  LocationLongitude                 DistributionChannel  \\\n",
      "0                Location Longitude                Distribution Channel   \n",
      "1  {\"ImportId\":\"locationLongitude\"}  {\"ImportId\":\"distributionChannel\"}   \n",
      "\n",
      "                  UserLanguage  \n",
      "0                User Language  \n",
      "1  {\"ImportId\":\"userLanguage\"}  , 'csv_filename': 'survey_responses_20250513_135954.csv', 'assignments': [], 'approved_count': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/princess/anaconda3/envs/stern/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Status</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Progress</th>\n",
       "      <th>Duration (in seconds)</th>\n",
       "      <th>Finished</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>RecipientLastName</th>\n",
       "      <th>RecipientFirstName</th>\n",
       "      <th>RecipientEmail</th>\n",
       "      <th>ExternalReference</th>\n",
       "      <th>LocationLatitude</th>\n",
       "      <th>LocationLongitude</th>\n",
       "      <th>DistributionChannel</th>\n",
       "      <th>UserLanguage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start Date</td>\n",
       "      <td>End Date</td>\n",
       "      <td>Response Type</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>Progress</td>\n",
       "      <td>Duration (in seconds)</td>\n",
       "      <td>Finished</td>\n",
       "      <td>Recorded Date</td>\n",
       "      <td>Response ID</td>\n",
       "      <td>Recipient Last Name</td>\n",
       "      <td>Recipient First Name</td>\n",
       "      <td>Recipient Email</td>\n",
       "      <td>External Data Reference</td>\n",
       "      <td>Location Latitude</td>\n",
       "      <td>Location Longitude</td>\n",
       "      <td>Distribution Channel</td>\n",
       "      <td>User Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"ImportId\":\"startDate\",\"timeZone\":\"Z\"}</td>\n",
       "      <td>{\"ImportId\":\"endDate\",\"timeZone\":\"Z\"}</td>\n",
       "      <td>{\"ImportId\":\"status\"}</td>\n",
       "      <td>{\"ImportId\":\"ipAddress\"}</td>\n",
       "      <td>{\"ImportId\":\"progress\"}</td>\n",
       "      <td>{\"ImportId\":\"duration\"}</td>\n",
       "      <td>{\"ImportId\":\"finished\"}</td>\n",
       "      <td>{\"ImportId\":\"recordedDate\",\"timeZone\":\"Z\"}</td>\n",
       "      <td>{\"ImportId\":\"_recordId\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientLastName\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientFirstName\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientEmail\"}</td>\n",
       "      <td>{\"ImportId\":\"externalDataReference\"}</td>\n",
       "      <td>{\"ImportId\":\"locationLatitude\"}</td>\n",
       "      <td>{\"ImportId\":\"locationLongitude\"}</td>\n",
       "      <td>{\"ImportId\":\"distributionChannel\"}</td>\n",
       "      <td>{\"ImportId\":\"userLanguage\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 StartDate  \\\n",
       "0                               Start Date   \n",
       "1  {\"ImportId\":\"startDate\",\"timeZone\":\"Z\"}   \n",
       "\n",
       "                                 EndDate                 Status  \\\n",
       "0                               End Date          Response Type   \n",
       "1  {\"ImportId\":\"endDate\",\"timeZone\":\"Z\"}  {\"ImportId\":\"status\"}   \n",
       "\n",
       "                  IPAddress                 Progress    Duration (in seconds)  \\\n",
       "0                IP Address                 Progress    Duration (in seconds)   \n",
       "1  {\"ImportId\":\"ipAddress\"}  {\"ImportId\":\"progress\"}  {\"ImportId\":\"duration\"}   \n",
       "\n",
       "                  Finished                                RecordedDate  \\\n",
       "0                 Finished                               Recorded Date   \n",
       "1  {\"ImportId\":\"finished\"}  {\"ImportId\":\"recordedDate\",\"timeZone\":\"Z\"}   \n",
       "\n",
       "                 ResponseId                 RecipientLastName  \\\n",
       "0               Response ID               Recipient Last Name   \n",
       "1  {\"ImportId\":\"_recordId\"}  {\"ImportId\":\"recipientLastName\"}   \n",
       "\n",
       "                  RecipientFirstName                 RecipientEmail  \\\n",
       "0               Recipient First Name                Recipient Email   \n",
       "1  {\"ImportId\":\"recipientFirstName\"}  {\"ImportId\":\"recipientEmail\"}   \n",
       "\n",
       "                      ExternalReference                 LocationLatitude  \\\n",
       "0               External Data Reference                Location Latitude   \n",
       "1  {\"ImportId\":\"externalDataReference\"}  {\"ImportId\":\"locationLatitude\"}   \n",
       "\n",
       "                  LocationLongitude                 DistributionChannel  \\\n",
       "0                Location Longitude                Distribution Channel   \n",
       "1  {\"ImportId\":\"locationLongitude\"}  {\"ImportId\":\"distributionChannel\"}   \n",
       "\n",
       "                  UserLanguage  \n",
       "0                User Language  \n",
       "1  {\"ImportId\":\"userLanguage\"}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell collects data from a completed survey\n",
    "def collect_survey_data():\n",
    "    # Get survey ID and HIT ID\n",
    "    survey_id = input(\"Enter your Qualtrics Survey ID: \")\n",
    "    hit_id = input(\"Enter your MTurk HIT ID: \")\n",
    "    \n",
    "    print(f\"Ready to collect data for Survey ID: {survey_id} and HIT ID: {hit_id}\")\n",
    "\n",
    "    # Create automation instance\n",
    "    automation = QualtricsAndMTurkAutomation()\n",
    "    \n",
    "    # Collect and process results\n",
    "    collected_data = automation.collect_and_process_results(\n",
    "        survey_id=survey_id,\n",
    "        hit_id=hit_id,\n",
    "        auto_approve=True \n",
    "    )\n",
    "\n",
    "    print(\"Data collection completed. Summary:\")\n",
    "    print(collected_data)\n",
    "\n",
    "    if 'responses' in collected_data:\n",
    "        display(collected_data['responses'])\n",
    "    else:\n",
    "        print(\"No responses collected.\")\n",
    "        \n",
    "    return collected_data\n",
    "\n",
    "# Run the function if executed directly\n",
    "collected_data = collect_survey_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
