{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.21 (main, Dec 11 2024, 10:21:40) \n",
      "[Clang 14.0.6 ]\n",
      "Warning: CrewAI requires Python 3.10 or higher. Attempting workaround...\n",
      "Failed to fix crewAI typing: unsupported operand type(s) for |: 'type' and 'NoneType'\n",
      "========================================\n",
      "⚠️  INPUT REQUIREMENTS:\n",
      "- You must include a line starting with 'Topic:'\n",
      "- You must include at least one line starting with 'Questions:'\n",
      "Otherwise, the survey cannot be processed.\n",
      "========================================\n",
      "\n",
      "===== Simplified Survey Generation =====\n",
      "Created survey on topic: The Theory of Planned Behavior Survey     Questions:     1. I intend to purchase organic food in the next month. (1=Strongly disagree; 7=Strongly agree)     2. Buying organic food is beneficial to my health. (1=Strongly disagree; 7=Strongly agree)     3. I feel confident in my ability to purchase organic food if I want to. (1=Strongly disagree; 7=Strongly agree)     4. People who are important to me think I should purchase organic food. (1=Strongly disagree; 7=Strongly agree)     5. The decision to buy organic food is entirely up to me. (1=Strongly disagree; 7=Strongly agree)\n",
      "Generated 0 questions\n",
      "Current working directory: /Users/princess/Documents/RA/Field-Experiment-AI-Agent\n",
      "Found .env file in current directory\n",
      "API Token loaded (masked): yNZ9********************************eAPM\n",
      "Data center: yul1\n",
      "Testing Qualtrics API connection...\n",
      "Connection successful! Authenticated as: Sichen Zhong\n",
      "MTurk client initialized in Sandbox mode\n",
      "MTurk account balance: $10000.00\n",
      "Starting Qualtrics and MTurk automation...\n",
      "Creating survey: The Theory of Planned Behavior Survey     Questions:     1. I intend to purchase organic food in the next month. (1=Strongly disagree; 7=Strongly agree)     2. Buying organic food is beneficial to my health. (1=Strongly disagree; 7=Strongly agree)     3. I feel confident in my ability to purchase organic food if I want to. (1=Strongly disagree; 7=Strongly agree)     4. People who are important to me think I should purchase organic food. (1=Strongly disagree; 7=Strongly agree)     5. The decision to buy organic food is entirely up to me. (1=Strongly disagree; 7=Strongly agree)\n",
      "Sending payload to Qualtrics: {\"SurveyName\": \"The Theory of Planned Behavior Survey     Questions:     1. I intend to purchase organic food in the next month. (1=Strongly disagree; 7=Strongly agree)     2. Buying organic food is b...\n",
      "Survey created successfully with ID: SV_3KHfvTBHt4Q1nrE\n",
      "Activating survey: SV_3KHfvTBHt4Q1nrE\n",
      "Survey activated successfully\n",
      "Creating distribution link for survey: SV_3KHfvTBHt4Q1nrE\n",
      "Anonymous survey link created: https://yul1.qualtrics.com/jfe/form/SV_3KHfvTBHt4Q1nrE\n",
      "Creating MTurk HIT with survey link\n",
      "HIT created successfully with ID: 3D06DR523VG90DWECRHZ2D7AUZNAMO\n",
      "Workers can access the HIT at: https://workersandbox.mturk.com/mturk/preview?groupId=34PM2WU5W0BJNKHPARJX9SUAXJ9ART\n",
      "\n",
      "Results:\n",
      "Survey ID: SV_3KHfvTBHt4Q1nrE\n",
      "Survey Link: https://yul1.qualtrics.com/jfe/form/SV_3KHfvTBHt4Q1nrE\n",
      "HIT ID: 3D06DR523VG90DWECRHZ2D7AUZNAMO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import Literal, Dict, List, Any, Union, Optional\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai.tasks.task_output import OutputFormat\n",
    "from crewai.knowledge.source.crew_docling_source import CrewDoclingSource\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "os.environ[\"OTEL_TRACES_EXPORTER\"] = \"none\"\n",
    "\n",
    "# ========== Pydantic Models ==========\n",
    "class ChoiceOption(BaseModel):\n",
    "    text: str\n",
    "    value: str\n",
    "\n",
    "class ChoiceConfig(BaseModel):\n",
    "    options: List[ChoiceOption]\n",
    "\n",
    "class SliderConfig(BaseModel):\n",
    "    min: float\n",
    "    max: float\n",
    "    step: float\n",
    "\n",
    "class TextInputConfig(BaseModel):\n",
    "    placeholder: Optional[str] = None\n",
    "    multiline: bool = False\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question_id: str\n",
    "    question_text: str\n",
    "    input_type: Literal[\"multiple_choice\", \"single_choice\", \"slider\", \"text_input\"]\n",
    "    input_config: Union[ChoiceConfig, SliderConfig, TextInputConfig]\n",
    "\n",
    "class Survey(BaseModel):\n",
    "    theme: str\n",
    "    purpose: str\n",
    "    questions: List[Question]\n",
    "\n",
    "class QuestionComment(BaseModel):\n",
    "    question_id: str\n",
    "    comment: str\n",
    "\n",
    "class AnnotatedSurvey(BaseModel):\n",
    "    survey: Survey\n",
    "    question_comments: List[QuestionComment]\n",
    "    overall_comment: Optional[str]\n",
    "\n",
    "class SurveyImprovementResult(BaseModel):\n",
    "    original_with_comments: AnnotatedSurvey\n",
    "    revised_survey: Survey\n",
    "\n",
    "# ========== Utility to load YAML ==========\n",
    "def load_yaml(path: str) -> dict:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# ========== Agent Definitions ==========\n",
    "def load_agents():\n",
    "    conv_cfg = load_yaml(\"config/agents/survey_convert_agent.yaml\")[\"survey_convert_agent\"]\n",
    "    convert_agent = Agent(\n",
    "        name=\"survey_convert_agent\",\n",
    "        role=conv_cfg[\"role\"],\n",
    "        goal=conv_cfg[\"goal\"],\n",
    "        backstory=conv_cfg[\"backstory\"],\n",
    "        verbose=conv_cfg[\"verbose\"],\n",
    "        allow_delegation=conv_cfg[\"allow_delegation\"]\n",
    "    )\n",
    "\n",
    "    edit_cfg = load_yaml(\"config/agents/survey_editor.yaml\")[\"survey_editor\"]\n",
    "    editor_agent = Agent(\n",
    "        name=\"survey_editor_agent\",\n",
    "        role=edit_cfg[\"role\"],\n",
    "        goal=edit_cfg[\"goal\"],\n",
    "        backstory=edit_cfg[\"backstory\"],\n",
    "        verbose=edit_cfg[\"verbose\"],\n",
    "        allow_delegation=edit_cfg[\"allow_delegation\"]\n",
    "    )\n",
    "    \n",
    "    return convert_agent, editor_agent\n",
    "\n",
    "# ========== Task Definitions ==========\n",
    "def load_tasks(convert_agent, editor_agent):\n",
    "    # Load convert task\n",
    "    conv_t = load_yaml(\"config/tasks/convert_survey_to_json.yaml\")[\"convert_survey_to_json\"]\n",
    "    convert_task = Task(\n",
    "        name=\"convert_survey_to_json\",\n",
    "        description=conv_t[\"description\"],\n",
    "        agent=convert_agent,\n",
    "        tool=conv_t.get(\"tool\"),\n",
    "        expected_output=conv_t[\"expected_output\"],\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "\n",
    "    # Load research task and manually replace placeholders\n",
    "    res_t = load_yaml(\"config/tasks/apply_survey_enhancements.yaml\")[\"research_task\"]\n",
    "    # Remove placeholders by replacing them with actual values or generic text\n",
    "    description = res_t[\"description\"].replace(\"{topic}\", \"the survey topic\").replace(\"{current_year}\", str(datetime.now().year))\n",
    "    expected_output = res_t[\"expected_output\"].replace(\"{topic}\", \"the survey topic\")\n",
    "    \n",
    "    research_task = Task(\n",
    "        name=\"research_task\",\n",
    "        description=description,\n",
    "        agent=convert_agent,\n",
    "        expected_output=expected_output,\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "\n",
    "    # Load comment task\n",
    "    com_t = load_yaml(\"config/tasks/comment_survey_task.yaml\")[\"comment_survey\"]\n",
    "    \n",
    "    comment_task = Task(\n",
    "        name=\"comment_survey\",\n",
    "        description=com_t[\"description\"],\n",
    "        agent=editor_agent,\n",
    "        expected_output=com_t[\"expected_output\"],\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "\n",
    "    # Load improve task\n",
    "    imp_t = load_yaml(\"config/tasks/apply_survey_enhancements.yaml\")[\"improve_survey\"]\n",
    "    # Handle the JSON schema example carefully\n",
    "    description = imp_t[\"description\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    expected_output = imp_t[\"expected_output\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    \n",
    "    improve_task = Task(\n",
    "        name=\"improve_survey\",\n",
    "        description=description,\n",
    "        agent=editor_agent,\n",
    "        expected_output=expected_output,\n",
    "        output_format=OutputFormat.JSON\n",
    "    )\n",
    "    \n",
    "    return convert_task, research_task, comment_task, improve_task\n",
    "\n",
    "def survey_dict_to_qualtrics_payload(survey_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a custom survey dict to a Qualtrics v3 API survey-definitions payload\n",
    "    Supports question types: multiple_choice, single_choice, slider, text_input\n",
    "    \"\"\"\n",
    "    survey_meta = survey_dict[\"revised_survey\"]\n",
    "    payload = {\n",
    "        \"SurveyName\":      survey_meta.get(\"theme\", \"New Survey\"),\n",
    "        \"Language\":        \"EN\",\n",
    "        \"ProjectCategory\": \"CORE\",\n",
    "        \"Questions\":       {}\n",
    "    }\n",
    "\n",
    "    for q in survey_meta[\"questions\"]:\n",
    "        raw_id = q[\"question_id\"]                  # e.g. \"q4\"\n",
    "        num    = re.sub(r'\\D+', '', raw_id)        # extract number \"4\"\n",
    "        qid    = f\"QID{num}\"                       # assemble \"QID4\"\n",
    "\n",
    "        qt  = q[\"question_text\"]\n",
    "        it  = q[\"input_type\"]\n",
    "        cfg = q[\"input_config\"]\n",
    "\n",
    "        # ---- Common fields ----\n",
    "        qobj = {\n",
    "            \"QuestionText\":      qt,\n",
    "            \"DataExportTag\":     qid,\n",
    "            \"Configuration\":     {\"QuestionDescriptionOption\": \"UseText\"},\n",
    "            \"Validation\":        {\"Settings\": {\"ForceResponse\": \"OFF\", \"Type\": \"None\"}}\n",
    "        }\n",
    "\n",
    "        # ---- Multiple/Single choice questions ----\n",
    "        if it in (\"multiple_choice\", \"single_choice\"):\n",
    "            choices = {}\n",
    "            for opt in cfg.get(\"options\", []):\n",
    "                if \"=\" in opt:\n",
    "                    idx, txt = opt.split(\"=\", 1)\n",
    "                    idx, txt = idx.strip(), txt.strip()\n",
    "                else:\n",
    "                    idx = str(len(choices) + 1)\n",
    "                    txt = opt.strip()\n",
    "                choices[idx] = {\"Display\": txt}\n",
    "\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"MC\",\n",
    "                \"Selector\":     \"SAVR\" if it == \"multiple_choice\" else \"SINGLE\",\n",
    "                \"SubSelector\":  \"TX\",\n",
    "                \"Choices\":      choices\n",
    "            })\n",
    "\n",
    "        # ---- Slider questions ----\n",
    "        elif it == \"slider\":\n",
    "            # Safely read slider parameters (default 0-100, step 1)\n",
    "            start = cfg.get(\"min\", cfg.get(\"start\", 0))\n",
    "            end   = cfg.get(\"max\", cfg.get(\"end\", 100))\n",
    "            step  = cfg.get(\"step\", cfg.get(\"stepSize\", 1))\n",
    "        \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"SL\",\n",
    "                \"Selector\":     \"Slider\",\n",
    "                \"SubSelector\":  \"SL\",\n",
    "                \"SliderStart\":  start,\n",
    "                \"SliderEnd\":    end,\n",
    "                \"SliderStep\":   step\n",
    "            })\n",
    "        # ---- Text input questions ----\n",
    "        elif it == \"text_input\":   \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"TE\",\n",
    "                \"Selector\":     \"ML\"   # Text Entry must use ML\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input_type: {it!r}\")\n",
    "\n",
    "        # Insert into final payload\n",
    "        payload[\"Questions\"][qid] = qobj\n",
    "\n",
    "    return payload\n",
    "\n",
    "class SurveyConversionOutput(BaseModel):\n",
    "    \"\"\"Model to validate the output of the survey conversion agent\"\"\"\n",
    "    title: str\n",
    "    fields: List[Dict[str, Any]]\n",
    "\n",
    "def validate_conversion_output(raw_output: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Validates the output of the survey conversion agent using Pydantic.\n",
    "    \n",
    "    Args:\n",
    "        raw_output: The raw JSON string output from the agent\n",
    "        \n",
    "    Returns:\n",
    "        The validated dictionary if successful\n",
    "        \n",
    "    Raises:\n",
    "        ValidationError: If the output doesn't match the expected schema\n",
    "        ValueError: If the output cannot be parsed as JSON\n",
    "    \"\"\"\n",
    "    # Clean up the raw output - strip markdown code blocks if present\n",
    "    cleaned_output = raw_output.strip()\n",
    "    if cleaned_output.startswith(\"```json\"):\n",
    "        cleaned_output = cleaned_output.split(\"```json\", 1)[1]\n",
    "    if cleaned_output.startswith(\"```\"):\n",
    "        cleaned_output = cleaned_output.split(\"```\", 1)[1]\n",
    "    if \"```\" in cleaned_output:\n",
    "        cleaned_output = cleaned_output.rsplit(\"```\", 1)[0]\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON\n",
    "        parsed_dict = json.loads(cleaned_output)\n",
    "        \n",
    "        # Validate using Pydantic model\n",
    "        validated = SurveyConversionOutput(**parsed_dict)\n",
    "        \n",
    "        # Return the validated dict\n",
    "        return validated.dict()\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Failed to parse JSON output: {e}\\nRaw output:\\n{raw_output}\")\n",
    "    except ValidationError as e:\n",
    "        raise ValidationError(f\"Output validation failed: {e}\\nRaw output:\\n{raw_output}\", SurveyConversionOutput)\n",
    "\n",
    "def convert_to_question_format(conversion_output: Dict) -> List[Question]:\n",
    "    \"\"\"\n",
    "    Converts the validated conversion output to a list of Question objects.\n",
    "    \n",
    "    Args:\n",
    "        conversion_output: The validated conversion output\n",
    "        \n",
    "    Returns:\n",
    "        List of Question objects\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for i, field in enumerate(conversion_output[\"fields\"]):\n",
    "        question_id = f\"q{i+1}\"\n",
    "        question_text = field[\"title\"]\n",
    "        \n",
    "        # Determine input type and config\n",
    "        if field[\"type\"] == \"multiple_choice\":\n",
    "            input_type = \"multiple_choice\"\n",
    "            options = [{\"text\": opt, \"value\": str(i)} for i, opt in enumerate(field.get(\"options\", []))]\n",
    "            input_config = ChoiceConfig(options=options)\n",
    "        elif field[\"type\"] == \"text_input\":\n",
    "            input_type = \"text_input\"\n",
    "            input_config = TextInputConfig(multiline=False)\n",
    "        elif field[\"type\"] == \"slider\":\n",
    "            input_type = \"slider\"\n",
    "            input_config = SliderConfig(min=0, max=100, step=1)  # Default values\n",
    "        else:\n",
    "            # Default to single_choice for most survey questions with scales\n",
    "            input_type = \"single_choice\"\n",
    "            options = [{\"text\": opt, \"value\": str(i)} for i, opt in enumerate(field.get(\"options\", []))]\n",
    "            input_config = ChoiceConfig(options=options)\n",
    "        \n",
    "        # Create Question object\n",
    "        question = Question(\n",
    "            question_id=question_id,\n",
    "            question_text=question_text,\n",
    "            input_type=input_type,\n",
    "            input_config=input_config\n",
    "        )\n",
    "        \n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Update the convert_task to include validation\n",
    "def modified_convert_task(conv_t, convert_agent):\n",
    "    \"\"\"Creates a modified convert task with validation\"\"\"\n",
    "    convert_task = Task(\n",
    "        name=\"convert_survey_to_json\",\n",
    "        description=conv_t[\"description\"],\n",
    "        agent=convert_agent,\n",
    "        tool=conv_t.get(\"tool\"),\n",
    "        expected_output=conv_t[\"expected_output\"],\n",
    "        output_format=OutputFormat.JSON,\n",
    "        async_execution=True,  # Enable async for better performance\n",
    "        validation_function=validate_conversion_output  # Add validation function\n",
    "    )\n",
    "    return convert_task\n",
    "\n",
    "class QualtricsClient:\n",
    "    \"\"\"Handles all Qualtrics API interactions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Qualtrics API client with credentials from .env file\"\"\"\n",
    "        # Print current working directory to help debug file path issues\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        \n",
    "        # Check if .env file exists\n",
    "        if os.path.exists('.env'):\n",
    "            print(\"Found .env file in current directory\")\n",
    "        else:\n",
    "            print(\"WARNING: No .env file found in current directory!\")\n",
    "            \n",
    "        # Load environment variables\n",
    "        load_dotenv(verbose=True)\n",
    "        \n",
    "        self.api_token = os.getenv('QUALTRICS_API_TOKEN')\n",
    "        self.data_center = os.getenv('QUALTRICS_DATA_CENTER')\n",
    "        self.directory_id = os.getenv('QUALTRICS_DIRECTORY_ID')\n",
    "        \n",
    "        # Print obfuscated token for debugging (only first/last 4 chars)\n",
    "        if self.api_token:\n",
    "            token_length = len(self.api_token)\n",
    "            masked_token = self.api_token[:4] + '*' * (token_length - 8) + self.api_token[-4:] if token_length > 8 else \"****\"\n",
    "            print(f\"API Token loaded (masked): {masked_token}\")\n",
    "        else:\n",
    "            print(\"WARNING: No API token found in environment variables!\")\n",
    "            \n",
    "        if self.data_center:\n",
    "            print(f\"Data center: {self.data_center}\")\n",
    "        else:\n",
    "            print(\"WARNING: No data center found in environment variables!\")\n",
    "        \n",
    "        if not self.api_token or not self.data_center:\n",
    "            raise ValueError(\"Missing Qualtrics API credentials in .env file\")\n",
    "            \n",
    "        # Set up base URL for API requests\n",
    "        self.base_url = f\"https://{self.data_center}.qualtrics.com/API/v3/\"\n",
    "        self.headers = {\n",
    "            \"X-API-Token\": self.api_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Test connection\n",
    "        print(\"Testing Qualtrics API connection...\")\n",
    "        try:\n",
    "            test_url = f\"{self.base_url}whoami\"\n",
    "            response = requests.get(test_url, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                user_info = response.json()[\"result\"]\n",
    "                print(f\"Connection successful! Authenticated as: {user_info.get('firstName', '')} {user_info.get('lastName', '')}\")\n",
    "            else:\n",
    "                print(f\"Connection test failed with status code: {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing connection: {str(e)}\")\n",
    "        \n",
    "    def create_survey(self, survey_name, survey_template=None):\n",
    "        \"\"\"\n",
    "        Create a new survey in Qualtrics\n",
    "        \n",
    "        Args:\n",
    "            survey_name (str): Name of the survey\n",
    "            survey_template (dict, optional): Survey template JSON\n",
    "            \n",
    "        Returns:\n",
    "            str: Survey ID of the created survey\n",
    "        \"\"\"\n",
    "        print(f\"Creating survey: {survey_name}\")\n",
    "        \n",
    "        # If no template is provided, use a basic template\n",
    "        if not survey_template:\n",
    "            # Define the survey payload with required fields including ProjectCategory\n",
    "            survey_payload = {\n",
    "                \"SurveyName\": survey_name,\n",
    "                \"Language\": \"EN\",\n",
    "                \"ProjectCategory\": \"CORE\", # Required field\n",
    "                \"Questions\": {\n",
    "                    \"QID1\": {\n",
    "                        \"QuestionText\": \"What is your age?\",\n",
    "                        \"QuestionType\": \"MC\",\n",
    "                        \"Selector\": \"SAVR\", # Required selector for multiple choice\n",
    "                        \"SubSelector\": \"TX\", # Text selector\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"Choices\": {\n",
    "                            \"1\": {\"Display\": \"18-24\"},\n",
    "                            \"2\": {\"Display\": \"25-34\"},\n",
    "                            \"3\": {\"Display\": \"35-44\"},\n",
    "                            \"4\": {\"Display\": \"45-54\"},\n",
    "                            \"5\": {\"Display\": \"55-64\"},\n",
    "                            \"6\": {\"Display\": \"65+\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"QID2\": {\n",
    "                        \"QuestionText\": \"How satisfied are you with our product?\",\n",
    "                        \"QuestionType\": \"Likert\",\n",
    "                        \"Selector\": \"LSL\", # Likert scale\n",
    "                        \"SubSelector\": \"TX\", # Text selector\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"Choices\": {\n",
    "                            \"1\": {\"Display\": \"Very dissatisfied\"},\n",
    "                            \"2\": {\"Display\": \"Dissatisfied\"},\n",
    "                            \"3\": {\"Display\": \"Neutral\"},\n",
    "                            \"4\": {\"Display\": \"Satisfied\"},\n",
    "                            \"5\": {\"Display\": \"Very satisfied\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"QID3\": {\n",
    "                        \"QuestionText\": \"Any additional comments?\",\n",
    "                        \"QuestionType\": \"TE\", # Text entry\n",
    "                        \"Selector\": \"ML\", # Multi-line\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # If a template is provided, make sure it includes ProjectCategory\n",
    "            survey_payload = survey_template\n",
    "            if \"ProjectCategory\" not in survey_payload:\n",
    "                survey_payload[\"ProjectCategory\"] = \"CORE\"\n",
    "        \n",
    "        # Create survey\n",
    "        url = f\"{self.base_url}survey-definitions\"\n",
    "        payload = json.dumps(survey_payload)\n",
    "        \n",
    "        print(f\"Sending payload to Qualtrics: {payload[:200]}...\")\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, data=payload)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error response: {response.text}\")\n",
    "            raise Exception(f\"Failed to create survey: {response.text}\")\n",
    "        \n",
    "        result = response.json()\n",
    "        survey_id = result[\"result\"][\"SurveyID\"]\n",
    "        print(f\"Survey created successfully with ID: {survey_id}\")\n",
    "        \n",
    "        return survey_id\n",
    "\n",
    "    def add_questions(self, survey_id: str, questions: List[dict]):\n",
    "        \"\"\"Add questions to a survey\"\"\"\n",
    "        for q in questions:\n",
    "            # Start with the fields every question needs\n",
    "            q_payload = {\n",
    "                 \"QuestionID\":   q[\"question_id\"],\n",
    "                 \"QuestionText\": q[\"question_text\"],\n",
    "                 \"QuestionType\": q[\"QuestionType\"],\n",
    "                 \"DataExportTag\": q[\"question_id\"],\n",
    "                 \"Configuration\": {\"QuestionDescriptionOption\": \"UseText\"},\n",
    "                 \"Validation\":    {\"Settings\": {\"ForceResponse\": \"OFF\", \"Type\": \"None\"}},\n",
    "             }\n",
    "\n",
    "            # Only add Selector/SubSelector if given\n",
    "            if \"Selector\" in q:\n",
    "                q_payload[\"Selector\"] = q[\"Selector\"]\n",
    "            if \"SubSelector\" in q:\n",
    "                q_payload[\"SubSelector\"] = q[\"SubSelector\"]\n",
    "            # Only add Choices if given\n",
    "            if \"Choices\" in q:\n",
    "                q_payload[\"Choices\"] = q[\"Choices\"]\n",
    "    \n",
    "            url = f\"{self.base_url}survey-definitions/{survey_id}/questions\"\n",
    "            resp = requests.post(url, headers=self.headers, json=q_payload)\n",
    "            print(f\"POST questions → {resp.status_code}\", resp.json())\n",
    "\n",
    "    def add_block(self, survey_id: str, block_payload: dict):\n",
    "        \"\"\"Add a block to a survey\"\"\"\n",
    "        url = f\"{self.base_url}survey-definitions/{survey_id}/blocks\"\n",
    "        resp = requests.post(url, headers=self.headers, json=block_payload)\n",
    "        print(f\"POST blocks → {resp.status_code}\", resp.json())\n",
    "\n",
    "    def update_flow(self, survey_id: str, flow_payload: dict):\n",
    "        \"\"\"Update the flow of a survey\"\"\"\n",
    "        url = f\"{self.base_url}survey-definitions/{survey_id}/flow\"\n",
    "        resp = requests.put(url, headers=self.headers, json=flow_payload)\n",
    "        print(\"PUT flow →\", resp.status_code, resp.json())\n",
    "    \n",
    "    def activate_survey(self, survey_id):\n",
    "        \"\"\"\n",
    "        Activate a survey to make it available for distribution\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey to activate\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful\n",
    "        \"\"\"\n",
    "        print(f\"Activating survey: {survey_id}\")\n",
    "        \n",
    "        url = f\"{self.base_url}surveys/{survey_id}\"\n",
    "        payload = json.dumps({\"isActive\": True})\n",
    "        \n",
    "        response = requests.put(url, headers=self.headers, data=payload)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to activate survey: {response.text}\")\n",
    "        \n",
    "        print(f\"Survey activated successfully\")\n",
    "        return True\n",
    "    \n",
    "    def create_distribution_link(self, survey_id, link_type=\"Anonymous\"):\n",
    "        \"\"\"\n",
    "        Create a distribution link for a survey\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey to distribute\n",
    "            link_type (str): Type of link (Anonymous or Individual)\n",
    "            \n",
    "        Returns:\n",
    "            str: Distribution link URL\n",
    "        \"\"\"\n",
    "        print(f\"Creating distribution link for survey: {survey_id}\")\n",
    "        \n",
    "        # For anonymous links, we can construct the URL directly based on the standard pattern\n",
    "        # https://DATACENTERID.qualtrics.com/jfe/form/SURVEYID\n",
    "        if link_type == \"Anonymous\":\n",
    "            survey_link = f\"https://{self.data_center}.qualtrics.com/jfe/form/{survey_id}\"\n",
    "            print(f\"Anonymous survey link created: {survey_link}\")\n",
    "            return survey_link\n",
    "        \n",
    "        # For other distribution types, we would use the API, but that's not implemented yet\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Distribution type '{link_type}' is not yet supported\")\n",
    "    \n",
    "    def get_survey_responses(self, survey_id, file_format=\"csv\"):\n",
    "        \"\"\"\n",
    "        Download survey responses\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey\n",
    "            file_format (str): Format of the response file (csv, json, spss, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Survey responses as a DataFrame\n",
    "        \"\"\"\n",
    "        print(f\"Downloading responses for survey: {survey_id}\")\n",
    "        \n",
    "        # Step 1: Create the export\n",
    "        export_url = f\"{self.base_url}surveys/{survey_id}/export-responses\"\n",
    "        export_payload = json.dumps({\n",
    "            \"format\": file_format,\n",
    "            \"useLabels\": True\n",
    "        })\n",
    "        \n",
    "        export_response = requests.post(export_url, headers=self.headers, data=export_payload)\n",
    "        \n",
    "        if export_response.status_code != 200:\n",
    "            raise Exception(f\"Failed to initiate export: {export_response.text}\")\n",
    "        \n",
    "        progress_id = export_response.json()[\"result\"][\"progressId\"]\n",
    "        \n",
    "        # Step 2: Check export progress\n",
    "        progress_status = \"inProgress\"\n",
    "        progress = 0\n",
    "        \n",
    "        while progress_status != \"complete\" and progress < 100:\n",
    "            progress_url = f\"{self.base_url}surveys/{survey_id}/export-responses/{progress_id}\"\n",
    "            progress_response = requests.get(progress_url, headers=self.headers)\n",
    "            \n",
    "            if progress_response.status_code != 200:\n",
    "                raise Exception(f\"Failed to check export progress: {progress_response.text}\")\n",
    "            \n",
    "            progress_result = progress_response.json()[\"result\"]\n",
    "            progress_status = progress_result[\"status\"]\n",
    "            progress = progress_result.get(\"percentComplete\", 0)\n",
    "            \n",
    "            print(f\"Export progress: {progress}%\")\n",
    "            \n",
    "            if progress_status != \"complete\" and progress < 100:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Step 3: Download the file\n",
    "        file_id = progress_result[\"fileId\"]\n",
    "        download_url = f\"{self.base_url}surveys/{survey_id}/export-responses/{file_id}/file\"\n",
    "        download_response = requests.get(download_url, headers=self.headers)\n",
    "        \n",
    "        if download_response.status_code != 200:\n",
    "            raise Exception(f\"Failed to download responses: {download_response.text}\")\n",
    "        \n",
    "        # Step 4: Extract and parse the zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(download_response.content)) as zip_file:\n",
    "            data_file = [f for f in zip_file.namelist() if f.endswith(f\".{file_format}\")][0]\n",
    "            with zip_file.open(data_file) as file:\n",
    "                if file_format == \"csv\":\n",
    "                    df = pd.read_csv(file)\n",
    "                elif file_format == \"json\":\n",
    "                    df = pd.read_json(file)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "        \n",
    "        print(f\"Successfully downloaded {len(df)} responses\")\n",
    "        return df\n",
    "\n",
    "class MTurkClient:\n",
    "    \"\"\"Handles all MTurk API interactions\"\"\"\n",
    "    def __init__(self, \n",
    "                aws_access_key_id: str = None, \n",
    "                aws_secret_access_key: str = None, \n",
    "                use_sandbox: bool = True):  # Default to sandbox mode for safety\n",
    "        \"\"\"\n",
    "        Initialize MTurk client\n",
    "        \n",
    "        Args:\n",
    "            aws_access_key_id: Optional override for AWS access key\n",
    "            aws_secret_access_key: Optional override for AWS secret key\n",
    "            use_sandbox: Boolean for using sandbox (defaults to True for safety)\n",
    "        \"\"\"\n",
    "        # Load from .env file\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Set AWS credentials (with optional overrides)\n",
    "        self.aws_access_key_id = aws_access_key_id or os.getenv('AWS_ACCESS_KEY_ID')\n",
    "        self.aws_secret_access_key = aws_secret_access_key or os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "        \n",
    "        # Check if credentials are available\n",
    "        if not self.aws_access_key_id or not self.aws_secret_access_key:\n",
    "            raise ValueError(\"Missing AWS credentials in .env file or constructor parameters\")\n",
    "\n",
    "        # Determine sandbox mode (with optional override)\n",
    "        if use_sandbox is None:\n",
    "            # Read from environment if not provided in constructor\n",
    "            self.use_sandbox = os.getenv('MTURK_SANDBOX', 'True').lower() == 'true'\n",
    "        else:\n",
    "            self.use_sandbox = use_sandbox\n",
    "\n",
    "        # Set endpoint based on sandbox mode\n",
    "        region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "        endpoint = (\n",
    "            'https://mturk-requester-sandbox.us-east-1.amazonaws.com'\n",
    "            if self.use_sandbox else\n",
    "            'https://mturk-requester.us-east-1.amazonaws.com'\n",
    "        )\n",
    "\n",
    "        # Create boto3 client\n",
    "        try:\n",
    "            self.client = boto3.client(\n",
    "                'mturk',\n",
    "                aws_access_key_id=self.aws_access_key_id,\n",
    "                aws_secret_access_key=self.aws_secret_access_key,\n",
    "                region_name=region,\n",
    "                endpoint_url=endpoint\n",
    "            )\n",
    "            print(f\"MTurk client initialized in {'Sandbox' if self.use_sandbox else 'Production'} mode\")\n",
    "            \n",
    "            # Verify connection by checking account balance\n",
    "            self.get_account_balance()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing MTurk client: {str(e)}\")\n",
    "            print(\"Please verify your AWS credentials and MTurk account configuration.\")\n",
    "            print(\"For MTurk integration, you need:\")\n",
    "            print(\"1. Valid AWS credentials in your .env file\")\n",
    "            print(\"2. Your AWS account linked to your MTurk Requester account\")\n",
    "            print(\"3. Proper permissions for the MTurk API\")\n",
    "            \n",
    "            # Create a dummy client for graceful degradation\n",
    "            self.client = None\n",
    "            self.connection_error = str(e)\n",
    "            \n",
    "    def get_account_balance(self):\n",
    "        \"\"\"Get the available MTurk account balance\"\"\"\n",
    "        if not self.client:\n",
    "            print(f\"Cannot check balance: {self.connection_error}\")\n",
    "            return 0.0\n",
    "            \n",
    "        try:\n",
    "            response = self.client.get_account_balance()\n",
    "            balance = response['AvailableBalance']\n",
    "            print(f\"MTurk account balance: ${balance}\")\n",
    "            return float(balance)\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking balance: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def create_hit_with_survey_link(self, survey_link, hit_config=None):\n",
    "        \"\"\"\n",
    "        Create an MTurk HIT with a link to a Qualtrics survey\n",
    "        \n",
    "        Args:\n",
    "            survey_link (str): URL to the Qualtrics survey\n",
    "            hit_config (dict, optional): Custom configuration for the HIT\n",
    "            \n",
    "        Returns:\n",
    "            str: HIT ID\n",
    "        \"\"\"\n",
    "        print(\"Creating MTurk HIT with survey link\")\n",
    "        \n",
    "        # Default HIT configuration\n",
    "        if not hit_config:\n",
    "            hit_config = {\n",
    "                'Title': 'Complete a short survey',\n",
    "                'Description': 'We need your input for a quick survey that should take less than 10 minutes',\n",
    "                'Keywords': 'survey, research, opinion, feedback',\n",
    "                'Reward': '0.50',\n",
    "                'MaxAssignments': 100,\n",
    "                'LifetimeInSeconds': 86400,  # 1 day\n",
    "                'AssignmentDurationInSeconds': 1800,  # 30 minutes\n",
    "                'AutoApprovalDelayInSeconds': 86400,  # 1 day\n",
    "                'QualificationRequirements': []\n",
    "            }\n",
    "        \n",
    "        # Create the HTML question with the survey link\n",
    "        question_html = f\"\"\"\n",
    "        <HTMLQuestion xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd\">\n",
    "            <HTMLContent><![CDATA[\n",
    "                <!DOCTYPE html>\n",
    "                <html>\n",
    "                <head>\n",
    "                    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
    "                    <script type='text/javascript' src='https://s3.amazonaws.com/mturk-public/externalHIT_v1.js'></script>\n",
    "                </head>\n",
    "                <body>\n",
    "                    <form name='mturk_form' method='post' id='mturk_form' action='https://www.mturk.com/mturk/externalSubmit'>\n",
    "                        <input type='hidden' value='' name='assignmentId' id='assignmentId'/>\n",
    "                        <h1>Survey Task</h1>\n",
    "                        <p>Please complete the survey at the following link:</p>\n",
    "                        <p><a href='{survey_link}' target='_blank'>{survey_link}</a></p>\n",
    "                        <p>After completing the survey, you will receive a completion code. Enter the code below:</p>\n",
    "                        <p><input type='text' name='completion_code' id='completion_code' size='40'/></p>\n",
    "                        <p><input type='submit' id='submitButton' value='Submit' /></p>\n",
    "                    </form>\n",
    "                    <script language='Javascript'>\n",
    "                        turkSetAssignmentID();\n",
    "                    </script>\n",
    "                </body>\n",
    "                </html>\n",
    "            ]]></HTMLContent>\n",
    "            <FrameHeight>600</FrameHeight>\n",
    "        </HTMLQuestion>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the HIT\n",
    "        response = self.client.create_hit(\n",
    "            Title=hit_config['Title'],\n",
    "            Description=hit_config['Description'],\n",
    "            Keywords=hit_config['Keywords'],\n",
    "            Reward=hit_config['Reward'],\n",
    "            MaxAssignments=hit_config['MaxAssignments'],\n",
    "            LifetimeInSeconds=hit_config['LifetimeInSeconds'],\n",
    "            AssignmentDurationInSeconds=hit_config['AssignmentDurationInSeconds'],\n",
    "            AutoApprovalDelayInSeconds=hit_config['AutoApprovalDelayInSeconds'],\n",
    "            Question=question_html,\n",
    "            QualificationRequirements=hit_config['QualificationRequirements']\n",
    "        )\n",
    "        \n",
    "        hit_id = response['HIT']['HITId']\n",
    "        hit_type_id = response['HIT']['HITTypeId']\n",
    "        \n",
    "        print(f\"HIT created successfully with ID: {hit_id}\")\n",
    "        \n",
    "        # Print the HIT URL\n",
    "        if self.use_sandbox:\n",
    "            worker_url = f\"https://workersandbox.mturk.com/mturk/preview?groupId={hit_type_id}\"\n",
    "        else:\n",
    "            worker_url = f\"https://worker.mturk.com/mturk/preview?groupId={hit_type_id}\"\n",
    "            \n",
    "        print(f\"Workers can access the HIT at: {worker_url}\")\n",
    "        \n",
    "        return hit_id\n",
    "    \n",
    "    def get_hit_assignments(self, hit_id):\n",
    "        \"\"\"\n",
    "        Get all assignments for a HIT\n",
    "        \n",
    "        Args:\n",
    "            hit_id (str): ID of the HIT\n",
    "            \n",
    "        Returns:\n",
    "            list: List of assignment dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"Getting assignments for HIT: {hit_id}\")\n",
    "        \n",
    "        # List to store all assignments\n",
    "        all_assignments = []\n",
    "        \n",
    "        # Get assignments with pagination\n",
    "        next_token = None\n",
    "        \n",
    "        while True:\n",
    "            if next_token:\n",
    "                response = self.client.list_assignments_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    NextToken=next_token,\n",
    "                    MaxResults=100\n",
    "                )\n",
    "            else:\n",
    "                response = self.client.list_assignments_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    MaxResults=100\n",
    "                )\n",
    "            \n",
    "            all_assignments.extend(response['Assignments'])\n",
    "            \n",
    "            if 'NextToken' in response:\n",
    "                next_token = response['NextToken']\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(all_assignments)} assignments\")\n",
    "        return all_assignments\n",
    "    \n",
    "    def approve_assignments(self, assignments, feedback=None):\n",
    "        \"\"\"\n",
    "        Approve multiple assignments\n",
    "        \n",
    "        Args:\n",
    "            assignments (list): List of assignment dictionaries or IDs\n",
    "            feedback (str, optional): Feedback to workers\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of successfully approved assignments\n",
    "        \"\"\"\n",
    "        approved_count = 0\n",
    "        \n",
    "        for assignment in assignments:\n",
    "            # Extract assignment ID if a dictionary was provided\n",
    "            assignment_id = assignment['AssignmentId'] if isinstance(assignment, dict) else assignment\n",
    "            \n",
    "            try:\n",
    "                self.client.approve_assignment(\n",
    "                    AssignmentId=assignment_id,\n",
    "                    RequesterFeedback=feedback if feedback else \"Thank you for your participation!\"\n",
    "                )\n",
    "                approved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error approving assignment {assignment_id}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Successfully approved {approved_count} assignments\")\n",
    "        return approved_count\n",
    "    \n",
    "    def delete_hit(self, hit_id):\n",
    "        \"\"\"\n",
    "        Delete a HIT\n",
    "        \n",
    "        Args:\n",
    "            hit_id (str): ID of the HIT to delete\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get the HIT status\n",
    "            hit = self.client.get_hit(HITId=hit_id)\n",
    "            status = hit['HIT']['HITStatus']\n",
    "            \n",
    "            # If the HIT is reviewable, dispose of it\n",
    "            if status == 'Reviewable':\n",
    "                self.client.delete_hit(HITId=hit_id)\n",
    "                print(f\"HIT {hit_id} deleted successfully\")\n",
    "                return True\n",
    "            \n",
    "            # If the HIT is assignable, expire it first then delete it\n",
    "            elif status == 'Assignable':\n",
    "                self.client.update_expiration_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    ExpireAt=datetime(2015, 1, 1)  # Set to a past date to expire immediately\n",
    "                )\n",
    "                time.sleep(1)  # Give time for the HIT to update\n",
    "                self.client.delete_hit(HITId=hit_id)\n",
    "                print(f\"HIT {hit_id} expired and deleted successfully\")\n",
    "                return True\n",
    "                \n",
    "            else:\n",
    "                print(f\"Cannot delete HIT {hit_id}, status is {status}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting HIT {hit_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Add the missing QualtricsAndMTurkAutomation class\n",
    "class QualtricsAndMTurkAutomation:\n",
    "    \"\"\"Handles the automation of creating Qualtrics surveys and MTurk HITs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the automation with Qualtrics and MTurk clients\"\"\"\n",
    "        self.qualtrics_client = QualtricsClient()\n",
    "        self.mturk_client = MTurkClient()\n",
    "    \n",
    "    def run(self, qualtrics_payload, hit_config=None):\n",
    "        \"\"\"\n",
    "        Run the automation to create a Qualtrics survey and MTurk HIT\n",
    "        \n",
    "        Args:\n",
    "            qualtrics_payload (dict): Survey definition for Qualtrics\n",
    "            hit_config (dict, optional): Configuration for MTurk HIT\n",
    "            \n",
    "        Returns:\n",
    "            dict: Results including survey ID, survey link, and HIT ID\n",
    "        \"\"\"\n",
    "        print(\"Starting Qualtrics and MTurk automation...\")\n",
    "        \n",
    "        # Create Qualtrics survey\n",
    "        survey_name = qualtrics_payload.get(\"SurveyName\", \"New Survey\")\n",
    "        survey_id = self.qualtrics_client.create_survey(survey_name, qualtrics_payload)\n",
    "        \n",
    "        # Activate the survey\n",
    "        self.qualtrics_client.activate_survey(survey_id)\n",
    "        \n",
    "        # Get distribution link\n",
    "        survey_link = self.qualtrics_client.create_distribution_link(survey_id)\n",
    "        \n",
    "        # Create MTurk HIT\n",
    "        hit_id = self.mturk_client.create_hit_with_survey_link(survey_link, hit_config)\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"survey_id\": survey_id,\n",
    "            \"survey_link\": survey_link,\n",
    "            \"hit_id\": hit_id\n",
    "        }\n",
    "    \n",
    "    def collect_and_process_results(self, survey_id, hit_id, auto_approve=True):\n",
    "        \"\"\"\n",
    "        Collect and process results from Qualtrics and MTurk\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): Qualtrics survey ID\n",
    "            hit_id (str): MTurk HIT ID\n",
    "            auto_approve (bool): Whether to automatically approve assignments\n",
    "            \n",
    "        Returns:\n",
    "            dict: Results including responses and assignment data\n",
    "        \"\"\"\n",
    "        # Get Qualtrics responses\n",
    "        responses = self.qualtrics_client.get_survey_responses(survey_id)\n",
    "        \n",
    "        # Save responses to CSV\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"survey_responses_{timestamp}.csv\"\n",
    "        responses.to_csv(csv_filename, index=False)\n",
    "        print(f\"Saved {len(responses)} responses to {csv_filename}\")\n",
    "        \n",
    "        # Get MTurk assignments\n",
    "        assignments = self.mturk_client.get_hit_assignments(hit_id)\n",
    "        \n",
    "        # Auto-approve assignments if requested\n",
    "        approved_count = 0\n",
    "        if auto_approve and assignments:\n",
    "            approved_count = self.mturk_client.approve_assignments(assignments)\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            \"responses\": responses,\n",
    "            \"csv_filename\": csv_filename,\n",
    "            \"assignments\": assignments,\n",
    "            \"approved_count\": approved_count\n",
    "        }\n",
    "\n",
    "# Fix the SurveyFlow class to work with crewAI's updated API\n",
    "from crewai import Flow\n",
    "\n",
    "class SurveyFlow(Flow):\n",
    "    \"\"\"Flow for processing a survey from text to deployment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the flow\"\"\"\n",
    "        super().__init__()\n",
    "        self._custom_state = {}  # Use a custom state dictionary instead\n",
    "        self.verbose = True\n",
    "    \n",
    "    async def kickoff_async(self, inputs):\n",
    "        \"\"\"Process survey and create Qualtrics survey and MTurk HIT\"\"\"\n",
    "        # Store inputs in our custom state\n",
    "        self._custom_state.update(inputs)\n",
    "        \n",
    "        # Get the survey text from state\n",
    "        survey_text = self._custom_state['survey_text'].strip()\n",
    "        first_line = survey_text.splitlines()[0]\n",
    "        topic = first_line.replace('Topic:', '').strip()\n",
    "        current_year = datetime.now().year\n",
    "\n",
    "        # Initialize agents and tasks\n",
    "        convert_agent, editor_agent = load_agents()\n",
    "        convert_task, research_task, comment_task, improve_task = load_tasks(convert_agent, editor_agent)\n",
    "        \n",
    "        # Create task input dictionary\n",
    "        task_inputs = {\n",
    "            'survey_text': survey_text,\n",
    "            'topic': topic,\n",
    "            'current_year': current_year\n",
    "        }\n",
    "        # Include knowledge. Everything in the knowledge file\n",
    "        content_source = CrewDoclingSource(\n",
    "            file_paths=[ \n",
    "                \"diamantopoulos-winklhofer-2001-index-construction-with-formative-indicators-an-alternative-to-scale-development.pdf\",\n",
    "                \"Marketing_survey_research_best_practice_2018.pdf\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Create the crew\n",
    "        survey_crew = Crew(\n",
    "            agents=[convert_agent, editor_agent],\n",
    "            tasks=[convert_task, research_task, comment_task, improve_task],\n",
    "            process=Process.sequential,\n",
    "            knowledge_sources=[content_source],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Run the crew to process the survey\n",
    "        crew_result = survey_crew.kickoff(\n",
    "            inputs=task_inputs\n",
    "        )\n",
    "\n",
    "        # Parse the result\n",
    "        raw = crew_result.raw.strip()\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "    \n",
    "        try:\n",
    "            survey_dict = json.loads(raw)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"JSON parsing failed: {e}\\nRaw output:\\n{raw}\")\n",
    "\n",
    "        # Print the results safely - enhanced with better error handling\n",
    "        print(\"\\n===== Survey Results =====\")\n",
    "        try:\n",
    "            # Try to get original survey with comments\n",
    "            if 'original_with_comments' in survey_dict:\n",
    "                print(\"\\n=== Original Survey (with comments) ===\")\n",
    "                annotated = survey_dict.get('original_with_comments', {})\n",
    "                survey = annotated.get('survey', {})\n",
    "                \n",
    "                # Print theme and purpose safely\n",
    "                print(f\"Theme: {survey.get('theme', 'N/A')}\")\n",
    "                print(f\"Purpose: {survey.get('purpose', 'N/A')}\\n\")\n",
    "                \n",
    "                # Safely iterate through questions\n",
    "                comments = annotated.get('question_comments', [])\n",
    "                for q in survey.get('questions', []):\n",
    "                    if isinstance(q, dict):\n",
    "                        qid = q.get('question_id', 'unknown')\n",
    "                        print(f\"Question {qid}: {q.get('question_text', 'N/A')}\")\n",
    "                        comment = next((c.get('comment', '') for c in comments if c.get('question_id') == qid), None)\n",
    "                        if comment:\n",
    "                            print(f\"  Comment: {comment}\")\n",
    "                        print()\n",
    "                \n",
    "                # Print overall comment if available\n",
    "                overall = annotated.get('overall_comment')\n",
    "                if overall:\n",
    "                    print(f\"Overall comment: {overall}\\n\")\n",
    "            \n",
    "            # Try to get revised survey\n",
    "            if 'revised_survey' in survey_dict:\n",
    "                print(\"=== Revised Survey ===\")\n",
    "                revised = survey_dict.get('revised_survey', {})\n",
    "                \n",
    "                # Print theme and purpose safely\n",
    "                print(f\"Theme:   {revised.get('theme', 'N/A')}\")\n",
    "                print(f\"Purpose: {revised.get('purpose', 'N/A')}\\n\")\n",
    "                \n",
    "                # Safely iterate through questions\n",
    "                for q in revised.get('questions', []):\n",
    "                    if isinstance(q, dict):\n",
    "                        qid = q.get('question_id', 'unknown')\n",
    "                        print(f\"Q{qid}: {q.get('question_text', 'N/A')}\")\n",
    "                        \n",
    "                        # Safely get options\n",
    "                        input_config = q.get('input_config', {})\n",
    "                        if isinstance(input_config, dict):\n",
    "                            opts = input_config.get('options', [])\n",
    "                            if opts:\n",
    "                                print(\"  Options:\")\n",
    "                                for o in opts:\n",
    "                                    print(f\"    - {o}\")\n",
    "                        print()\n",
    "            \n",
    "            # If standard format is not found, print raw structure\n",
    "            if 'original_with_comments' not in survey_dict and 'revised_survey' not in survey_dict:\n",
    "                print(\"Survey output doesn't match expected structure. Raw output:\")\n",
    "                print(json.dumps(survey_dict, indent=2))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing survey structure: {str(e)}\")\n",
    "            print(\"Raw survey data:\")\n",
    "            print(json.dumps(survey_dict, indent=2))\n",
    "            \n",
    "        # Store the survey dict in our custom state\n",
    "        self._custom_state['survey_dict'] = survey_dict\n",
    "        return survey_dict\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the survey processing and deployment flow\"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Display input requirements\n",
    "    print(\"========================================\")\n",
    "    print(\"⚠️  INPUT REQUIREMENTS:\")\n",
    "    print(\"- You must include a line starting with 'Topic:'\")\n",
    "    print(\"- You must include at least one line starting with 'Questions:'\")\n",
    "    print(\"Otherwise, the survey cannot be processed.\")\n",
    "    print(\"========================================\")\n",
    "    \n",
    "    # Get survey input\n",
    "    survey_to_process = input(\"Please enter the Survey content: \")\n",
    "\n",
    "    # Initialize and run the flow - FIXED: completely removed parameters\n",
    "    flow = SurveyFlow()\n",
    "    \n",
    "    # Execute the flow using nest_asyncio to handle Jupyter's event loop\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    # Now we can use asyncio.run inside Jupyter\n",
    "    survey_dict = asyncio.run(flow.kickoff_async(inputs={\n",
    "        'survey_text': survey_to_process\n",
    "    }))\n",
    "\n",
    "    # Convert the survey to Qualtrics format\n",
    "    qualtrics_payload = survey_dict_to_qualtrics_payload(survey_dict)\n",
    "\n",
    "    # Create HIT configuration\n",
    "    hit_config = {\n",
    "        'Title': 'Complete a short survey on organic food',\n",
    "        'Description': survey_dict[\"revised_survey\"][\"purpose\"],\n",
    "        'Keywords': 'survey, research, feedback',\n",
    "        'Reward': '0.75',\n",
    "        'MaxAssignments': 100,\n",
    "        'LifetimeInSeconds': 86400,\n",
    "        'AssignmentDurationInSeconds': 1800,\n",
    "        'AutoApprovalDelayInSeconds': 86400,\n",
    "        'QualificationRequirements': []\n",
    "    }\n",
    "    \n",
    "    # Run the automation to create the survey and HIT\n",
    "    automation = QualtricsAndMTurkAutomation()\n",
    "    results = automation.run(qualtrics_payload, hit_config)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Survey ID: {results['survey_id']}\")\n",
    "    print(f\"Survey Link: {results['survey_link']}\")\n",
    "    print(f\"HIT ID: {results['hit_id']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8c35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to collect data for Survey ID: SV_3KHfvTBHt4Q1nrE and HIT ID: 3D06DR523VG90DWECRHZ2D7AUZNAMO\n",
      "Current working directory: /Users/princess/Documents/RA/Field-Experiment-AI-Agent\n",
      "Found .env file in current directory\n",
      "API Token loaded (masked): yNZ9********************************eAPM\n",
      "Data center: yul1\n",
      "Testing Qualtrics API connection...\n",
      "Connection successful! Authenticated as: Sichen Zhong\n",
      "MTurk client initialized in Sandbox mode\n",
      "MTurk account balance: $10000.00\n",
      "Downloading responses for survey: SV_3KHfvTBHt4Q1nrE\n",
      "Export progress: 0.0%\n",
      "Export progress: 0.0%\n",
      "Export progress: 100.0%\n",
      "Successfully downloaded 2 responses\n",
      "Saved 2 responses to survey_responses_20250513_211630.csv\n",
      "Getting assignments for HIT: 3D06DR523VG90DWECRHZ2D7AUZNAMO\n",
      "Found 0 assignments\n",
      "Data collection completed. Summary:\n",
      "{'responses':                                  StartDate  \\\n",
      "0                               Start Date   \n",
      "1  {\"ImportId\":\"startDate\",\"timeZone\":\"Z\"}   \n",
      "\n",
      "                                 EndDate                 Status  \\\n",
      "0                               End Date          Response Type   \n",
      "1  {\"ImportId\":\"endDate\",\"timeZone\":\"Z\"}  {\"ImportId\":\"status\"}   \n",
      "\n",
      "                  IPAddress                 Progress    Duration (in seconds)  \\\n",
      "0                IP Address                 Progress    Duration (in seconds)   \n",
      "1  {\"ImportId\":\"ipAddress\"}  {\"ImportId\":\"progress\"}  {\"ImportId\":\"duration\"}   \n",
      "\n",
      "                  Finished                                RecordedDate  \\\n",
      "0                 Finished                               Recorded Date   \n",
      "1  {\"ImportId\":\"finished\"}  {\"ImportId\":\"recordedDate\",\"timeZone\":\"Z\"}   \n",
      "\n",
      "                 ResponseId                 RecipientLastName  \\\n",
      "0               Response ID               Recipient Last Name   \n",
      "1  {\"ImportId\":\"_recordId\"}  {\"ImportId\":\"recipientLastName\"}   \n",
      "\n",
      "                  RecipientFirstName                 RecipientEmail  \\\n",
      "0               Recipient First Name                Recipient Email   \n",
      "1  {\"ImportId\":\"recipientFirstName\"}  {\"ImportId\":\"recipientEmail\"}   \n",
      "\n",
      "                      ExternalReference                 LocationLatitude  \\\n",
      "0               External Data Reference                Location Latitude   \n",
      "1  {\"ImportId\":\"externalDataReference\"}  {\"ImportId\":\"locationLatitude\"}   \n",
      "\n",
      "                  LocationLongitude                 DistributionChannel  \\\n",
      "0                Location Longitude                Distribution Channel   \n",
      "1  {\"ImportId\":\"locationLongitude\"}  {\"ImportId\":\"distributionChannel\"}   \n",
      "\n",
      "                  UserLanguage  \n",
      "0                User Language  \n",
      "1  {\"ImportId\":\"userLanguage\"}  , 'csv_filename': 'survey_responses_20250513_211630.csv', 'assignments': [], 'approved_count': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Status</th>\n",
       "      <th>IPAddress</th>\n",
       "      <th>Progress</th>\n",
       "      <th>Duration (in seconds)</th>\n",
       "      <th>Finished</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>RecipientLastName</th>\n",
       "      <th>RecipientFirstName</th>\n",
       "      <th>RecipientEmail</th>\n",
       "      <th>ExternalReference</th>\n",
       "      <th>LocationLatitude</th>\n",
       "      <th>LocationLongitude</th>\n",
       "      <th>DistributionChannel</th>\n",
       "      <th>UserLanguage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start Date</td>\n",
       "      <td>End Date</td>\n",
       "      <td>Response Type</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>Progress</td>\n",
       "      <td>Duration (in seconds)</td>\n",
       "      <td>Finished</td>\n",
       "      <td>Recorded Date</td>\n",
       "      <td>Response ID</td>\n",
       "      <td>Recipient Last Name</td>\n",
       "      <td>Recipient First Name</td>\n",
       "      <td>Recipient Email</td>\n",
       "      <td>External Data Reference</td>\n",
       "      <td>Location Latitude</td>\n",
       "      <td>Location Longitude</td>\n",
       "      <td>Distribution Channel</td>\n",
       "      <td>User Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"ImportId\":\"startDate\",\"timeZone\":\"Z\"}</td>\n",
       "      <td>{\"ImportId\":\"endDate\",\"timeZone\":\"Z\"}</td>\n",
       "      <td>{\"ImportId\":\"status\"}</td>\n",
       "      <td>{\"ImportId\":\"ipAddress\"}</td>\n",
       "      <td>{\"ImportId\":\"progress\"}</td>\n",
       "      <td>{\"ImportId\":\"duration\"}</td>\n",
       "      <td>{\"ImportId\":\"finished\"}</td>\n",
       "      <td>{\"ImportId\":\"recordedDate\",\"timeZone\":\"Z\"}</td>\n",
       "      <td>{\"ImportId\":\"_recordId\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientLastName\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientFirstName\"}</td>\n",
       "      <td>{\"ImportId\":\"recipientEmail\"}</td>\n",
       "      <td>{\"ImportId\":\"externalDataReference\"}</td>\n",
       "      <td>{\"ImportId\":\"locationLatitude\"}</td>\n",
       "      <td>{\"ImportId\":\"locationLongitude\"}</td>\n",
       "      <td>{\"ImportId\":\"distributionChannel\"}</td>\n",
       "      <td>{\"ImportId\":\"userLanguage\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 StartDate  \\\n",
       "0                               Start Date   \n",
       "1  {\"ImportId\":\"startDate\",\"timeZone\":\"Z\"}   \n",
       "\n",
       "                                 EndDate                 Status  \\\n",
       "0                               End Date          Response Type   \n",
       "1  {\"ImportId\":\"endDate\",\"timeZone\":\"Z\"}  {\"ImportId\":\"status\"}   \n",
       "\n",
       "                  IPAddress                 Progress    Duration (in seconds)  \\\n",
       "0                IP Address                 Progress    Duration (in seconds)   \n",
       "1  {\"ImportId\":\"ipAddress\"}  {\"ImportId\":\"progress\"}  {\"ImportId\":\"duration\"}   \n",
       "\n",
       "                  Finished                                RecordedDate  \\\n",
       "0                 Finished                               Recorded Date   \n",
       "1  {\"ImportId\":\"finished\"}  {\"ImportId\":\"recordedDate\",\"timeZone\":\"Z\"}   \n",
       "\n",
       "                 ResponseId                 RecipientLastName  \\\n",
       "0               Response ID               Recipient Last Name   \n",
       "1  {\"ImportId\":\"_recordId\"}  {\"ImportId\":\"recipientLastName\"}   \n",
       "\n",
       "                  RecipientFirstName                 RecipientEmail  \\\n",
       "0               Recipient First Name                Recipient Email   \n",
       "1  {\"ImportId\":\"recipientFirstName\"}  {\"ImportId\":\"recipientEmail\"}   \n",
       "\n",
       "                      ExternalReference                 LocationLatitude  \\\n",
       "0               External Data Reference                Location Latitude   \n",
       "1  {\"ImportId\":\"externalDataReference\"}  {\"ImportId\":\"locationLatitude\"}   \n",
       "\n",
       "                  LocationLongitude                 DistributionChannel  \\\n",
       "0                Location Longitude                Distribution Channel   \n",
       "1  {\"ImportId\":\"locationLongitude\"}  {\"ImportId\":\"distributionChannel\"}   \n",
       "\n",
       "                  UserLanguage  \n",
       "0                User Language  \n",
       "1  {\"ImportId\":\"userLanguage\"}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell collects data from a completed survey\n",
    "def collect_survey_data():\n",
    "    # Get survey ID and HIT ID\n",
    "    survey_id = input(\"Enter your Qualtrics Survey ID: \")\n",
    "    hit_id = input(\"Enter your MTurk HIT ID: \")\n",
    "    \n",
    "    print(f\"Ready to collect data for Survey ID: {survey_id} and HIT ID: {hit_id}\")\n",
    "\n",
    "    # Create automation instance\n",
    "    automation = QualtricsAndMTurkAutomation()\n",
    "    \n",
    "    # Collect and process results\n",
    "    collected_data = automation.collect_and_process_results(\n",
    "        survey_id=survey_id,\n",
    "        hit_id=hit_id,\n",
    "        auto_approve=True \n",
    "    )\n",
    "\n",
    "    print(\"Data collection completed. Summary:\")\n",
    "    print(collected_data)\n",
    "\n",
    "    if 'responses' in collected_data:\n",
    "        display(collected_data['responses'])\n",
    "    else:\n",
    "        print(\"No responses collected.\")\n",
    "        \n",
    "    return collected_data\n",
    "\n",
    "# Run the function if executed directly\n",
    "collected_data = collect_survey_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
