{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e744665e-7420-433f-8463-a674b66492d5",
   "metadata": {},
   "source": [
    "# Survey Distribution & Collection Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d92d726c-e144-490b-aa95-27f90c556582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import xml.etree.ElementTree as ET\n",
    "from botocore.config import Config\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']     = \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \n",
    "os.environ['AWS_REGION']            = \n",
    "\n",
    "config = Config(region_name='us-east-1', signature_version='v4')\n",
    "mturk = boto3.client(\n",
    "    'mturk',\n",
    "    config=config,\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "    endpoint_url='https://mturk-requester-sandbox.us-east-1.amazonaws.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b49b921e-a6cd-4b29-9416-76293a13ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HIT: 34XASH8KMGJGNH2G1AZWS8W5TGIMP1\n"
     ]
    }
   ],
   "source": [
    "def create_survey_hit(survey_link: str,\n",
    "                      reward: str = \"0.01\",\n",
    "                      max_assignments: int = 1) -> str:\n",
    "    xml_question = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<ExternalQuestion xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2006-07-14/ExternalQuestion.xsd\">\n",
    "  <ExternalURL>{survey_link}</ExternalURL>\n",
    "  <FrameHeight>600</FrameHeight>\n",
    "</ExternalQuestion>\"\"\"\n",
    "    resp = mturk.create_hit(\n",
    "        Title=\"Sandbox Survey Test\",\n",
    "        Description=\"è¯·åœ¨æ­¤ HIT ä¸­æ‰“å¼€é—®å·å¹¶å¡«å†™\",\n",
    "        Reward=reward,\n",
    "        MaxAssignments=max_assignments,\n",
    "        LifetimeInSeconds=3600,\n",
    "        AssignmentDurationInSeconds=1800,\n",
    "        Question=xml_question\n",
    "    )\n",
    "    hit_id = resp['HIT']['HITId']\n",
    "    print(f\"Created HIT: {hit_id}\")\n",
    "    return hit_id\n",
    "\n",
    "survey_link = \"https://qualtricsxmnznfsjncm.qualtrics.com/jfe/form/SV_00tpW4sa8vcRYma\"\n",
    "hit_id = create_survey_hit(survey_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5da1fedf-6cae-4a24-8c23-eb58408690c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘‰ è¯·åœ¨ https://workersandbox.mturk.com/ ç”¨ Sandbox Worker è´¦å·é¢†å–å¹¶æäº¤è¯¥ HIT åŽï¼ŒæŒ‰å›žè½¦ç»§ç»­...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nðŸ‘‰ è¯·åœ¨ https://workersandbox.mturk.com/ ç”¨ Sandbox Worker è´¦å·é¢†å–å¹¶æäº¤è¯¥ HIT åŽï¼ŒæŒ‰å›žè½¦ç»§ç»­...\")\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa0e924-6960-4965-a2f4-450126f33678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ å°šæœªæ”¶åˆ°ä»»ä½•æäº¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "def list_assignments(hit_id: str):\n",
    "    resp = mturk.list_assignments_for_hit(\n",
    "        HITId=hit_id,\n",
    "        AssignmentStatuses=['Submitted', 'Approved', 'Rejected'],\n",
    "        MaxResults=100\n",
    "    )\n",
    "    return resp['Assignments']\n",
    "\n",
    "def parse_external_answer(answer_xml: str) -> dict:\n",
    "    root = ET.fromstring(answer_xml)\n",
    "    result = {}\n",
    "    for ans in root.findall('.//AnswerField'):\n",
    "        qid = ans.findtext('QuestionIdentifier')\n",
    "        text = ans.findtext('FreeText')\n",
    "        result[qid] = text\n",
    "    return result\n",
    "\n",
    "assignments = list_assignments(hit_id)\n",
    "if not assignments:\n",
    "    print(\"âš ï¸ å°šæœªæ”¶åˆ°ä»»ä½•æäº¤ã€‚\")\n",
    "else:\n",
    "    for a in assignments:\n",
    "        print(\"â€•â€•â€• Assignment ID:\", a['AssignmentId'])\n",
    "        print(\"    Worker ID:    \", a['WorkerId'])\n",
    "        print(\"    Answerï¼š\", parse_external_answer(a['Answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbd1c01-ecc2-4dc4-b845-847c610bc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attmept achiving this with agents\n",
    "\n",
    "# publish_tool = PublishSurveyTool()\n",
    "# fetch_tool   = FetchSurveyTool()\n",
    "\n",
    "# deploy_agent  = Agent(config=deploy_cfg,  tools=[publish_tool])\n",
    "# collect_agent = Agent(config=collect_cfg, tools=[fetch_tool])\n",
    "\n",
    "# post_task = Task(\n",
    "#     config=post_cfg,\n",
    "#     agent=deploy_agent,\n",
    "#     run=publish_tool.run\n",
    "# )\n",
    "# get_task = Task(\n",
    "#     config=get_cfg,\n",
    "#     agent=collect_agent,\n",
    "#     run=fetch_tool.run\n",
    "# )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     crew = Crew(\n",
    "#         agents=[deploy_agent, collect_agent],\n",
    "#         tasks =[post_task, get_task],\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     survey_definition = {\n",
    "#         \"title\": \"Customer Satisfaction Survey\",\n",
    "#         \"fields\": [\n",
    "#             {\n",
    "#                 \"title\": \"How satisfied are you with our service?\",\n",
    "#                 \"type\": \"multiple_choice\",\n",
    "#                 \"properties\": {\n",
    "#                     \"choices\": [\n",
    "#                         {\"label\": \"Very Satisfied\"},\n",
    "#                         {\"label\": \"Satisfied\"},\n",
    "#                         {\"label\": \"Neutral\"},\n",
    "#                         {\"label\": \"Dissatisfied\"}\n",
    "#                     ]\n",
    "#                 }\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "#     direct_pub = publish_tool.run(survey_definition)\n",
    "#     print(\"Direct publish output:\", direct_pub)\n",
    "#     fid = direct_pub[\"id\"]\n",
    "#     direct_fetch = fetch_tool.run(fid)\n",
    "#     print(\"Direct fetch output:\", direct_fetch)\n",
    "\n",
    "#     # print(\"\\n=== Using Crew.kickoff ===\")\n",
    "#     # deploy_out  = crew.kickoff(inputs={\"survey_definition\": survey_definition})\n",
    "#     # print(\"Crew deployed:\", deploy_out)\n",
    "#     # fid = deploy_out[\"id\"]\n",
    "#     # collect_out = crew.kickoff(inputs={\"survey_id\": fid})\n",
    "#     # print(\"Crew collected:\", collect_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392b8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: research_task not found in config/tasks/apply_survey_enhancements.yaml. Using default configuration.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Flow.__init__() got an unexpected keyword argument 'agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 273\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_dict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m survey_dict\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m survey_dict\n\u001b[0;32m--> 273\u001b[0m flow \u001b[38;5;241m=\u001b[39m SurveyFlow(\n\u001b[1;32m    274\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[convert_agent, editor_agent],\n\u001b[1;32m    275\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[convert_task, research_task, improve_task],\n\u001b[1;32m    276\u001b[0m     process\u001b[38;5;241m=\u001b[39mProcess\u001b[38;5;241m.\u001b[39msequential,\n\u001b[1;32m    277\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    278\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msurvey_dict_to_qualtrics_payload\u001b[39m(survey_dict: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    input survey_dict\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m      - theme(for SurveyName)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    Qualtrics v3 API çš„ survey-definitions JSON\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Flow.__init__() got an unexpected keyword argument 'agents'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Union, Literal, Optional, Dict, Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from crewai import Agent, Task, Crew, Flow, Process\n",
    "from crewai.flow.flow import start\n",
    "from crewai.tasks.task_output import OutputFormat\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "logging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "os.environ[\"OTEL_TRACES_EXPORTER\"] = \"none\"\n",
    "\n",
    "# ========== Pydantic Models ==========\n",
    "class ChoiceOption(BaseModel):\n",
    "    text: str\n",
    "    value: str\n",
    "\n",
    "class ChoiceConfig(BaseModel):\n",
    "    options: List[ChoiceOption]\n",
    "\n",
    "class SliderConfig(BaseModel):\n",
    "    min: float\n",
    "    max: float\n",
    "    step: float\n",
    "    \n",
    "class MatrixRowOption(BaseModel):\n",
    "    text: str\n",
    "    value: str\n",
    "\n",
    "class MatrixColumnOption(BaseModel):\n",
    "    text: str\n",
    "    value: str\n",
    "\n",
    "class MatrixConfig(BaseModel):\n",
    "    rows: List[MatrixRowOption]\n",
    "    columns: List[MatrixColumnOption]\n",
    "\n",
    "class TextInputConfig(BaseModel):\n",
    "    placeholder: Optional[str] = None\n",
    "    multiline: bool = False\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question_id: str\n",
    "    question_text: str\n",
    "    input_type: Literal[\"multiple_choice\", \"single_choice\", \"slider\", \"text_input\", \"matrix\", \"text_entry\", \"descriptive_text\", \"rank_order\", \"side_by_side\", \"drill_down\", \"meta_info\", \"constant_sum\", \"timing\", \"heat_map\", \"net_promoter\", \"graphic_slider\"]\n",
    "    input_config: Union[ChoiceConfig, SliderConfig, TextInputConfig, MatrixConfig]\n",
    "\n",
    "class Survey(BaseModel):\n",
    "    theme: str\n",
    "    purpose: str\n",
    "    questions: List[Question]\n",
    "\n",
    "class QuestionComment(BaseModel):\n",
    "    question_id: str\n",
    "    comment: str\n",
    "\n",
    "class AnnotatedSurvey(BaseModel):\n",
    "    survey: Survey\n",
    "    question_comments: List[QuestionComment]\n",
    "    overall_comment: Optional[str]\n",
    "\n",
    "class SurveyImprovementResult(BaseModel):\n",
    "    original_with_comments: AnnotatedSurvey\n",
    "    revised_survey: Survey\n",
    "\n",
    "# ========== Utility to load YAML ==========\n",
    "def load_yaml(path: str) -> dict:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# ========== Agent Definitions ==========\n",
    "def load_agent_config(yaml_path, agent_key, default_config=None):\n",
    "    \"\"\"Safely load agent configuration with fallback to default\"\"\"\n",
    "    try:\n",
    "        config = load_yaml(yaml_path)\n",
    "        if agent_key in config:\n",
    "            return config[agent_key]\n",
    "        else:\n",
    "            print(f\"Warning: {agent_key} not found in {yaml_path}. Using default configuration.\")\n",
    "            return default_config or {}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {yaml_path} not found. Using default configuration.\")\n",
    "        return default_config or {}\n",
    "\n",
    "# Default agent configurations\n",
    "default_convert_agent_config = {\n",
    "    \"role\": \"Survey Conversion Assistant\",\n",
    "    \"goal\": \"Convert survey text to structured JSON format\",\n",
    "    \"backstory\": \"Expert at parsing survey data\",\n",
    "    \"verbose\": True,\n",
    "    \"allow_delegation\": False\n",
    "}\n",
    "\n",
    "default_editor_agent_config = {\n",
    "    \"role\": \"Survey Editor\",\n",
    "    \"goal\": \"Improve survey quality and structure\",\n",
    "    \"backstory\": \"Expert at survey design and improvement\",\n",
    "    \"verbose\": True,\n",
    "    \"allow_delegation\": False\n",
    "}\n",
    "\n",
    "# Load agent configurations with fallbacks\n",
    "conv_cfg = load_agent_config(\"config/agents/survey_convert_agent.yaml\", \"survey_convert_agent\", default_convert_agent_config)\n",
    "convert_agent = Agent(\n",
    "    name=\"survey_convert_agent\",\n",
    "    role=conv_cfg[\"role\"],\n",
    "    goal=conv_cfg[\"goal\"],\n",
    "    backstory=conv_cfg[\"backstory\"],\n",
    "    verbose=conv_cfg[\"verbose\"],\n",
    "    allow_delegation=conv_cfg[\"allow_delegation\"]\n",
    ")\n",
    "\n",
    "edit_cfg = load_agent_config(\"config/agents/survey_editor.yaml\", \"survey_editor\", default_editor_agent_config)\n",
    "editor_agent = Agent(\n",
    "    name=\"survey_editor_agent\",\n",
    "    role=edit_cfg[\"role\"],\n",
    "    goal=edit_cfg[\"goal\"],\n",
    "    backstory=edit_cfg[\"backstory\"],\n",
    "    verbose=edit_cfg[\"verbose\"],\n",
    "    allow_delegation=edit_cfg[\"allow_delegation\"]\n",
    ")\n",
    "\n",
    "# ========== Task Definitions ==========\n",
    "def load_task_config(yaml_path, task_key, default_config=None):\n",
    "    \"\"\"Safely load task configuration with fallback to default\"\"\"\n",
    "    try:\n",
    "        config = load_yaml(yaml_path)\n",
    "        if task_key in config:\n",
    "            return config[task_key]\n",
    "        else:\n",
    "            print(f\"Warning: {task_key} not found in {yaml_path}. Using default configuration.\")\n",
    "            return default_config or {}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {yaml_path} not found. Using default configuration.\")\n",
    "        return default_config or {}\n",
    "\n",
    "# Default task configurations\n",
    "default_convert_config = {\n",
    "    \"description\": \"Convert survey text to JSON format\",\n",
    "    \"expected_output\": \"JSON-formatted survey data\",\n",
    "    \"inputs\": {},\n",
    "    \"outputs\": {}\n",
    "}\n",
    "\n",
    "default_research_config = {\n",
    "    \"description\": \"Research survey best practices and make recommendations\",\n",
    "    \"expected_output\": \"Survey enhancement recommendations\",\n",
    "    \"inputs\": {},\n",
    "    \"outputs\": {}\n",
    "}\n",
    "\n",
    "default_improve_config = {\n",
    "    \"description\": \"Apply survey improvements based on recommendations\",\n",
    "    \"expected_output\": \"Improved survey JSON\",\n",
    "    \"inputs\": {},\n",
    "    \"outputs\": {}\n",
    "}\n",
    "\n",
    "# Load task configurations with fallbacks\n",
    "conv_t = load_task_config(\"config/tasks/convert_survey_to_json.yaml\", \"convert_survey_to_json\", default_convert_config)\n",
    "convert_task = Task(\n",
    "    name=\"convert_survey_to_json\",\n",
    "    description=conv_t[\"description\"],\n",
    "    agent=convert_agent,\n",
    "    tool=conv_t.get(\"tool\"),\n",
    "    inputs=list(conv_t.get(\"inputs\", {}).keys()),\n",
    "    outputs=list(conv_t.get(\"outputs\", {}).keys()),\n",
    "    expected_output=conv_t[\"expected_output\"],\n",
    "    output_format=OutputFormat.JSON\n",
    ")\n",
    "\n",
    "res_t = load_task_config(\"config/tasks/apply_survey_enhancements.yaml\", \"research_task\", default_research_config)\n",
    "research_task = Task(\n",
    "    name=\"research_task\",\n",
    "    description=res_t[\"description\"],\n",
    "    agent=convert_agent,\n",
    "    inputs=list(res_t.get(\"inputs\", {}).keys()),\n",
    "    outputs=list(res_t.get(\"outputs\", {}).keys()),\n",
    "    expected_output=res_t[\"expected_output\"],\n",
    "    output_format=OutputFormat.JSON\n",
    ")\n",
    "\n",
    "imp_t = load_task_config(\"config/tasks/apply_survey_enhancements.yaml\", \"improve_survey\", default_improve_config)\n",
    "improve_task = Task(\n",
    "    name=\"improve_survey\",\n",
    "    description=imp_t[\"description\"],\n",
    "    agent=editor_agent,\n",
    "    inputs=list(imp_t.get(\"inputs\", {}).keys()),\n",
    "    outputs=list(imp_t.get(\"outputs\", {}).keys()),\n",
    "    expected_output=imp_t[\"expected_output\"],\n",
    "    output_format=OutputFormat.JSON\n",
    ")\n",
    "\n",
    "# ========== Crew Definition ==========\n",
    "survey_crew = Crew(\n",
    "    agents=[convert_agent, editor_agent],\n",
    "    tasks=[convert_task, research_task, improve_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ========== Flow Definition ==========\n",
    "class SurveyFlow(Flow):\n",
    "    @start()\n",
    "    def initial_run(self):\n",
    "        survey_text = self.state['survey_text'].strip()\n",
    "        first_line = survey_text.splitlines()[0]\n",
    "        topic = first_line.replace('Topic:', '').strip()\n",
    "        current_year = datetime.now().year\n",
    "\n",
    "        crew_result = survey_crew.kickoff(\n",
    "            inputs={\n",
    "                'survey_text': survey_text,\n",
    "                'topic': topic,\n",
    "                'current_year': current_year\n",
    "            }\n",
    "        )\n",
    "\n",
    "        raw = crew_result.raw.strip()\n",
    "        if raw.startswith('```') and raw.endswith('```'):\n",
    "            raw = raw.split('\\n', 1)[1].rsplit('```', 1)[0]\n",
    "        try:\n",
    "            survey_dict = json.loads(raw)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"è§£æž JSON å¤±è´¥: {e}\\nRaw output:\\n{raw}\")\n",
    "\n",
    "        # Pretty-print the original survey with comments\n",
    "        annotated = survey_dict.get('original_with_comments', {})\n",
    "        survey = annotated.get('survey', {})\n",
    "        comments = annotated.get('question_comments', [])\n",
    "        print(\"\\n=== Original Survey (with comments) ===\")\n",
    "        print(f\"Theme: {survey.get('theme', '')}\")\n",
    "        print(f\"Purpose: {survey.get('purpose', '')}\\n\")\n",
    "        for q in survey.get('questions', []):\n",
    "            qid = q.get('question_id')\n",
    "            print(f\"Question {qid}: {q.get('question_text')}\")\n",
    "            comment = next((c['comment'] for c in comments if c['question_id'] == qid), None)\n",
    "            if comment:\n",
    "                print(f\"  Comment: {comment}\")\n",
    "            print()\n",
    "        overall = annotated.get('overall_comment')\n",
    "        if overall:\n",
    "            print(f\"Overall comment: {overall}\\n\")\n",
    "            \n",
    "        revised = survey_dict.get('revised_survey', {})\n",
    "        print(\"=== Revised Survey ===\")\n",
    "        print(f\"Theme:   {revised['theme']}\")\n",
    "        print(f\"Purpose: {revised['purpose']}\\n\")\n",
    "        for q in revised['questions']:\n",
    "            print(f\"Q{q['question_id']}: {q['question_text']}\")\n",
    "            opts = q['input_config'].get('options')\n",
    "            if opts:\n",
    "                print(\"  Options:\")\n",
    "                for o in opts:\n",
    "                    print(f\"    - {o}\")\n",
    "            print()\n",
    "        self.state['survey_dict'] = survey_dict\n",
    "        return survey_dict\n",
    "\n",
    "        \n",
    "flow = SurveyFlow(\n",
    "    agents=[convert_agent, editor_agent],\n",
    "    tasks=[convert_task, research_task, improve_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "def survey_dict_to_qualtrics_payload(survey_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    input survey_dict\n",
    "      - theme(for SurveyName)\n",
    "      - purpose (Description)\n",
    "      - questions: List of { question_id, question_text, input_type, input_config }\n",
    "    Qualtrics v3 API çš„ survey-definitions JSON\n",
    "    \"\"\"\n",
    "\n",
    "    survey_meta = survey_dict[\"revised_survey\"] \n",
    "    payload = {\n",
    "        \"SurveyName\": survey_meta.get(\"theme\", \"New Survey\"),\n",
    "        \"Language\": \"EN\",\n",
    "        \"ProjectCategory\": \"CORE\",\n",
    "        \"Questions\": {}\n",
    "    }\n",
    "    questions = survey_meta[\"questions\"]\n",
    "    for q in questions:\n",
    "        qid = q[\"question_id\"]\n",
    "        qt = q[\"question_text\"]\n",
    "        it = q[\"input_type\"]\n",
    "        cfg = q[\"input_config\"]\n",
    "\n",
    "        qobj = {\n",
    "            \"QuestionText\": qt,\n",
    "            \"Configuration\": {\"QuestionDescriptionOption\": \"UseText\"},\n",
    "            \"Validation\": {\"Settings\": {\"ForceResponse\": \"OFF\", \"Type\": \"None\"}}\n",
    "        }\n",
    "\n",
    "        if it in (\"multiple_choice\", \"single_choice\"):\n",
    "            choices_dict = {}\n",
    "            for opt in cfg[\"options\"]:\n",
    "                if \"=\" in opt:\n",
    "                    index, text = opt.split(\"=\", 1)\n",
    "                    choices_dict[index.strip()] = {\"Display\": text.strip()}\n",
    "                else:\n",
    "                    idx = str(len(choices_dict)+1)\n",
    "                    choices_dict[idx] = {\"Display\": opt.strip()}\n",
    "            \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"MC\",\n",
    "                \"Selector\": \"SAVR\" if it == \"multiple_choice\" else \"SINGLE\",\n",
    "                \"SubSelector\": \"TX\",\n",
    "                \"Choices\": choices_dict\n",
    "            })\n",
    "\n",
    "        elif it == \"slider\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"SL\",\n",
    "                \"Selector\": \"Slider\",\n",
    "                \"SubSelector\": \"SL\"\n",
    "            })\n",
    "         \n",
    "        elif it == \"text_input\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"TE\",\n",
    "                \"Selector\": \"ML\" if cfg.get(\"multiline\", False) else \"TX\"\n",
    "            })\n",
    "            \n",
    "        elif it == \"matrix\":\n",
    "            # Matrix table question (Bipolar)\n",
    "            choices_dict = {}\n",
    "            answs_dict = {}\n",
    "            for idx, row in enumerate(cfg[\"rows\"], 1):\n",
    "                answs_dict[str(idx)] = {\"Display\": row[\"text\"]}\n",
    "            for idx, col in enumerate(cfg[\"columns\"], 1):\n",
    "                choices_dict[str(idx)] = {\"Display\": col[\"text\"]}\n",
    "            \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"Matrix\",\n",
    "                \"Selector\": \"Bipolar\",\n",
    "                \"SubSelector\": \"SingleAnswer\",\n",
    "                \"Choices\": choices_dict,\n",
    "                \"Answers\": answs_dict\n",
    "            })\n",
    "            \n",
    "        elif it == \"text_entry\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"TE\",\n",
    "                \"Selector\": cfg.get(\"selector\", \"TX\"),\n",
    "                \"TextBoxFormat\": cfg.get(\"format\", None)\n",
    "            })\n",
    "            \n",
    "        elif it == \"descriptive_text\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"DB\",\n",
    "                \"Selector\": \"TX\"\n",
    "            })\n",
    "            \n",
    "        elif it == \"rank_order\":\n",
    "            choices_dict = {}\n",
    "            for idx, opt in enumerate(cfg[\"options\"], 1):\n",
    "                choices_dict[str(idx)] = {\"Display\": opt[\"text\"]}\n",
    "            \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"RO\",\n",
    "                \"Selector\": \"OD\",\n",
    "                \"Choices\": choices_dict\n",
    "            })\n",
    "            \n",
    "        elif it == \"side_by_side\":\n",
    "            choices_dict = {}\n",
    "            for idx, opt in enumerate(cfg[\"options\"], 1):\n",
    "                choices_dict[str(idx)] = {\"Display\": opt[\"text\"]}\n",
    "            \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"SBS\",\n",
    "                \"Selector\": \"MultipleTextBox\",\n",
    "                \"Choices\": choices_dict\n",
    "            })\n",
    "            \n",
    "        elif it == \"constant_sum\":\n",
    "            choices_dict = {}\n",
    "            for idx, opt in enumerate(cfg[\"options\"], 1):\n",
    "                choices_dict[str(idx)] = {\"Display\": opt[\"text\"]}\n",
    "            \n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"CS\",\n",
    "                \"Selector\": \"CSS\",\n",
    "                \"Choices\": choices_dict,\n",
    "                \"Configuration\": {\n",
    "                    \"TotalSum\": cfg.get(\"total_sum\", 100)\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        elif it == \"heat_map\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"HM\",\n",
    "                \"Selector\": \"SelectedTable\",\n",
    "                \"HeatMapImage\": cfg.get(\"image_url\", \"\"),\n",
    "                \"HeatMapHeight\": cfg.get(\"height\", 600),\n",
    "                \"HeatMapWidth\": cfg.get(\"width\", 800)\n",
    "            })\n",
    "            \n",
    "        elif it == \"graphic_slider\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"GR\",\n",
    "                \"Selector\": \"One\",\n",
    "                \"SubSelector\": \"Slider\",\n",
    "                \"GraphicLeft\": cfg.get(\"left_text\", \"Unsatisfied\"),\n",
    "                \"GraphicRight\": cfg.get(\"right_text\", \"Satisfied\"),\n",
    "                \"GraphicImageUp\": cfg.get(\"image_up_url\", \"\"),\n",
    "                \"GraphicImageDown\": cfg.get(\"image_down_url\", \"\")\n",
    "            })\n",
    "            \n",
    "        elif it == \"net_promoter\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"NPS\",\n",
    "                \"Selector\": \"StandardNPS\",\n",
    "                \"Configuration\": {\n",
    "                    \"QuestionDescriptionOption\": \"UseText\",\n",
    "                    \"Format\": {\n",
    "                        \"TextBefore\": cfg.get(\"text_before\", \"How likely are you to recommend us?\"),\n",
    "                        \"TextAfter\": cfg.get(\"text_after\", \"\")\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        elif it == \"timing\":\n",
    "            qobj.update({\n",
    "                \"QuestionType\": \"Timing\",\n",
    "                \"Selector\": \"PageTimer\",\n",
    "                \"Hidden\": True\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input_type: {it}\")\n",
    "        \n",
    "        payload[\"Questions\"][qid] = qobj\n",
    "\n",
    "    return payload\n",
    "\n",
    "class QualtricsClient:\n",
    "    \"\"\"Handles all Qualtrics API interactions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Qualtrics API client with credentials from .env file\"\"\"\n",
    "        # Print current working directory to help debug file path issues\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        \n",
    "        # Check if .env file exists\n",
    "        if os.path.exists('.env'):\n",
    "            print(\"Found .env file in current directory\")\n",
    "        else:\n",
    "            print(\"WARNING: No .env file found in current directory!\")\n",
    "            \n",
    "        # Load environment variables\n",
    "        load_dotenv(verbose=True)\n",
    "        \n",
    "        self.api_token = os.getenv('QUALTRICS_API_TOKEN')\n",
    "        self.data_center = os.getenv('QUALTRICS_DATA_CENTER')\n",
    "        self.directory_id = os.getenv('QUALTRICS_DIRECTORY_ID')\n",
    "        \n",
    "        # Print obfuscated token for debugging (only first/last 4 chars)\n",
    "        if self.api_token:\n",
    "            token_length = len(self.api_token)\n",
    "            masked_token = self.api_token[:4] + '*' * (token_length - 8) + self.api_token[-4:] if token_length > 8 else \"****\"\n",
    "            print(f\"API Token loaded (masked): {masked_token}\")\n",
    "        else:\n",
    "            print(\"WARNING: No API token found in environment variables!\")\n",
    "            \n",
    "        if self.data_center:\n",
    "            print(f\"Data center: {self.data_center}\")\n",
    "        else:\n",
    "            print(\"WARNING: No data center found in environment variables!\")\n",
    "        \n",
    "        if not self.api_token or not self.data_center:\n",
    "            raise ValueError(\"Missing Qualtrics API credentials in .env file\")\n",
    "            \n",
    "        # Set up base URL for API requests\n",
    "        self.base_url = f\"https://{self.data_center}.qualtrics.com/API/v3/\"\n",
    "        self.headers = {\n",
    "            \"X-API-Token\": self.api_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Test connection\n",
    "        print(\"Testing Qualtrics API connection...\")\n",
    "        try:\n",
    "            test_url = f\"{self.base_url}whoami\"\n",
    "            response = requests.get(test_url, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                user_info = response.json()[\"result\"]\n",
    "                print(f\"Connection successful! Authenticated as: {user_info.get('firstName', '')} {user_info.get('lastName', '')}\")\n",
    "            else:\n",
    "                print(f\"Connection test failed with status code: {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing connection: {str(e)}\")\n",
    "        \n",
    "    def create_survey(self, survey_name, survey_template=None):\n",
    "        \"\"\"\n",
    "        Create a new survey in Qualtrics\n",
    "        \n",
    "        Args:\n",
    "            survey_name (str): Name of the survey\n",
    "            survey_template (dict, optional): Survey template JSON\n",
    "            \n",
    "        Returns:\n",
    "            str: Survey ID of the created survey\n",
    "        \"\"\"\n",
    "        print(f\"Creating survey: {survey_name}\")\n",
    "        \n",
    "        # If no template is provided, use a basic template\n",
    "        if not survey_template:\n",
    "            # Define the survey payload with required fields including ProjectCategory\n",
    "            survey_payload = {\n",
    "                \"SurveyName\": survey_name,\n",
    "                \"Language\": \"EN\",\n",
    "                \"ProjectCategory\": \"CORE\", # This is the required field that was missing\n",
    "                \"Questions\": {\n",
    "                    \"QID1\": {\n",
    "                        \"QuestionText\": \"What is your age?\",\n",
    "                        \"QuestionType\": \"MC\",\n",
    "                        \"Selector\": \"SAVR\", # Required selector for multiple choice questions\n",
    "                        \"SubSelector\": \"TX\", # Text selector\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"Choices\": {\n",
    "                            \"1\": {\"Display\": \"18-24\"},\n",
    "                            \"2\": {\"Display\": \"25-34\"},\n",
    "                            \"3\": {\"Display\": \"35-44\"},\n",
    "                            \"4\": {\"Display\": \"45-54\"},\n",
    "                            \"5\": {\"Display\": \"55-64\"},\n",
    "                            \"6\": {\"Display\": \"65+\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"QID2\": {\n",
    "                        \"QuestionText\": \"How satisfied are you with our product?\",\n",
    "                        \"QuestionType\": \"Likert\",\n",
    "                        \"Selector\": \"LSL\", # Likert scale\n",
    "                        \"SubSelector\": \"TX\", # Text selector\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"Choices\": {\n",
    "                            \"1\": {\"Display\": \"Very dissatisfied\"},\n",
    "                            \"2\": {\"Display\": \"Dissatisfied\"},\n",
    "                            \"3\": {\"Display\": \"Neutral\"},\n",
    "                            \"4\": {\"Display\": \"Satisfied\"},\n",
    "                            \"5\": {\"Display\": \"Very satisfied\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"QID3\": {\n",
    "                        \"QuestionText\": \"Any additional comments?\",\n",
    "                        \"QuestionType\": \"TE\", # Text entry\n",
    "                        \"Selector\": \"ML\", # Multi-line\n",
    "                        \"Configuration\": {\n",
    "                            \"QuestionDescriptionOption\": \"UseText\"\n",
    "                        },\n",
    "                        \"Validation\": {\n",
    "                            \"Settings\": {\n",
    "                                \"ForceResponse\": \"OFF\",\n",
    "                                \"Type\": \"None\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            # If a template is provided, make sure it includes ProjectCategory\n",
    "            survey_payload = survey_template\n",
    "            if \"ProjectCategory\" not in survey_payload:\n",
    "                survey_payload[\"ProjectCategory\"] = \"CORE\"\n",
    "        \n",
    "        # Create survey\n",
    "        url = f\"{self.base_url}survey-definitions\"\n",
    "        payload = json.dumps(survey_payload)\n",
    "        \n",
    "        print(f\"Sending payload to Qualtrics: {payload[:200]}...\")\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, data=payload)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error response: {response.text}\")\n",
    "            raise Exception(f\"Failed to create survey: {response.text}\")\n",
    "        \n",
    "        result = response.json()\n",
    "        survey_id = result[\"result\"][\"SurveyID\"]\n",
    "        print(f\"Survey created successfully with ID: {survey_id}\")\n",
    "        \n",
    "        return survey_id\n",
    "\n",
    "    def add_questions(self, survey_id: str, questions: List[dict]):\n",
    "        for q in questions:\n",
    "            # start with the fields every question needs\n",
    "            q_payload = {\n",
    "                \"QuestionID\":   q[\"question_id\"],\n",
    "                \"QuestionText\": q[\"question_text\"],\n",
    "                \"QuestionType\": q[\"QuestionType\"],\n",
    "                \"Configuration\": {\"QuestionDescriptionOption\": \"UseText\"},\n",
    "                \"Validation\":    {\"Settings\": {\"ForceResponse\": \"OFF\", \"Type\": \"None\"}},\n",
    "            }\n",
    "            # only add Selector/SubSelector if given\n",
    "            if \"Selector\" in q:\n",
    "                q_payload[\"Selector\"] = q[\"Selector\"]\n",
    "            if \"SubSelector\" in q:\n",
    "                q_payload[\"SubSelector\"] = q[\"SubSelector\"]\n",
    "            # only add Choices if given\n",
    "            if \"Choices\" in q:\n",
    "                q_payload[\"Choices\"] = q[\"Choices\"]\n",
    "    \n",
    "            url = f\"{self.base_url}survey-definitions/{survey_id}/questions\"\n",
    "            resp = requests.post(url, headers=self.headers, json=q_payload)\n",
    "            print(f\"POST questions â†’ {resp.status_code}\", resp.json())\n",
    "\n",
    "    def add_block(self, survey_id: str, block_payload: dict):\n",
    "        url = f\"{self.base_url}survey-definitions/{survey_id}/blocks\"\n",
    "        resp = requests.post(url, headers=self.headers, json=block_payload)\n",
    "        print(f\"POST blocks â†’ {resp.status_code}\", resp.json())\n",
    "\n",
    "    def update_flow(self, survey_id: str, flow_payload: dict):\n",
    "        url = f\"{self.base_url}survey-definitions/{survey_id}/flow\"\n",
    "        resp = requests.put(url, headers=self.headers, json=flow_payload)\n",
    "        print(\"PUT flow â†’\", resp.status_code, resp.json())\n",
    "    \n",
    "    def activate_survey(self, survey_id):\n",
    "        \"\"\"\n",
    "        Activate a survey to make it available for distribution\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey to activate\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful\n",
    "        \"\"\"\n",
    "        print(f\"Activating survey: {survey_id}\")\n",
    "        \n",
    "        url = f\"{self.base_url}surveys/{survey_id}\"\n",
    "        payload = json.dumps({\"isActive\": True})\n",
    "        \n",
    "        response = requests.put(url, headers=self.headers, data=payload)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to activate survey: {response.text}\")\n",
    "        \n",
    "        print(f\"Survey activated successfully\")\n",
    "        return True\n",
    "    \n",
    "    def create_distribution_link(self, survey_id, link_type=\"Anonymous\"):\n",
    "        \"\"\"\n",
    "        Create a distribution link for a survey\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey to distribute\n",
    "            link_type (str): Type of link (Anonymous or Individual)\n",
    "            \n",
    "        Returns:\n",
    "            str: Distribution link URL\n",
    "        \"\"\"\n",
    "        print(f\"Creating distribution link for survey: {survey_id}\")\n",
    "        \n",
    "        # For anonymous links, we can construct the URL directly based on the standard pattern\n",
    "        # https://DATACENTERID.qualtrics.com/jfe/form/SURVEYID\n",
    "        if link_type == \"Anonymous\":\n",
    "            survey_link = f\"https://{self.data_center}.qualtrics.com/jfe/form/{survey_id}\"\n",
    "            print(f\"Anonymous survey link created: {survey_link}\")\n",
    "            return survey_link\n",
    "        \n",
    "        # For other distribution types, we would use the API, but that's not implemented yet\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Distribution type '{link_type}' is not yet supported\")\n",
    "    \n",
    "    def get_survey_responses(self, survey_id, file_format=\"csv\"):\n",
    "        \"\"\"\n",
    "        Download survey responses\n",
    "        \n",
    "        Args:\n",
    "            survey_id (str): ID of the survey\n",
    "            file_format (str): Format of the response file (csv, json, spss, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Survey responses as a DataFrame\n",
    "        \"\"\"\n",
    "        print(f\"Downloading responses for survey: {survey_id}\")\n",
    "        \n",
    "        # Step 1: Create the export\n",
    "        export_url = f\"{self.base_url}surveys/{survey_id}/export-responses\"\n",
    "        export_payload = json.dumps({\n",
    "            \"format\": file_format,\n",
    "            \"useLabels\": True\n",
    "        })\n",
    "        \n",
    "        export_response = requests.post(export_url, headers=self.headers, data=export_payload)\n",
    "        \n",
    "        if export_response.status_code != 200:\n",
    "            raise Exception(f\"Failed to initiate export: {export_response.text}\")\n",
    "        \n",
    "        progress_id = export_response.json()[\"result\"][\"progressId\"]\n",
    "        \n",
    "        # Step 2: Check export progress\n",
    "        progress_status = \"inProgress\"\n",
    "        progress = 0\n",
    "        \n",
    "        while progress_status != \"complete\" and progress < 100:\n",
    "            progress_url = f\"{self.base_url}surveys/{survey_id}/export-responses/{progress_id}\"\n",
    "            progress_response = requests.get(progress_url, headers=self.headers)\n",
    "            \n",
    "            if progress_response.status_code != 200:\n",
    "                raise Exception(f\"Failed to check export progress: {progress_response.text}\")\n",
    "            \n",
    "            progress_result = progress_response.json()[\"result\"]\n",
    "            progress_status = progress_result[\"status\"]\n",
    "            progress = progress_result.get(\"percentComplete\", 0)\n",
    "            \n",
    "            print(f\"Export progress: {progress}%\")\n",
    "            \n",
    "            if progress_status != \"complete\" and progress < 100:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Step 3: Download the file\n",
    "        file_id = progress_result[\"fileId\"]\n",
    "        download_url = f\"{self.base_url}surveys/{survey_id}/export-responses/{file_id}/file\"\n",
    "        download_response = requests.get(download_url, headers=self.headers)\n",
    "        \n",
    "        if download_response.status_code != 200:\n",
    "            raise Exception(f\"Failed to download responses: {download_response.text}\")\n",
    "        \n",
    "        # Step 4: Extract and parse the zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(download_response.content)) as zip_file:\n",
    "            data_file = [f for f in zip_file.namelist() if f.endswith(f\".{file_format}\")][0]\n",
    "            with zip_file.open(data_file) as file:\n",
    "                if file_format == \"csv\":\n",
    "                    df = pd.read_csv(file)\n",
    "                elif file_format == \"json\":\n",
    "                    df = pd.read_json(file)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "        \n",
    "        print(f\"Successfully downloaded {len(df)} responses\")\n",
    "        return df\n",
    "\n",
    "class MTurkClient:\n",
    "    \"\"\"Handles all MTurk API interactions\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        aws_access_key_id: str = None,\n",
    "        aws_secret_access_key: str = None,\n",
    "        use_sandbox: bool = True\n",
    "        ):\n",
    "\n",
    "        if aws_access_key_id and aws_secret_access_key:\n",
    "            self.aws_access_key_id     = aws_access_key_id\n",
    "            self.aws_secret_access_key = aws_secret_access_key\n",
    "        else:\n",
    "            self.aws_access_key_id     = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "            self.aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "        self.use_sandbox = use_sandbox\n",
    "\n",
    "        if not self.aws_access_key_id or not self.aws_secret_access_key:\n",
    "            raise ValueError(\"Missing AWS credentials\")\n",
    "\n",
    "        region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "        endpoint = (\n",
    "            'https://mturk-requester-sandbox.us-east-1.amazonaws.com'\n",
    "            if self.use_sandbox else\n",
    "            'https://mturk-requester.us-east-1.amazonaws.com'\n",
    "        )\n",
    "\n",
    "        self.client = boto3.client(\n",
    "            'mturk',\n",
    "            aws_access_key_id=self.aws_access_key_id,\n",
    "            aws_secret_access_key=self.aws_secret_access_key,\n",
    "            region_name=region,\n",
    "            endpoint_url=endpoint\n",
    "        )\n",
    "        print(f\"MTurk client initialized in {'Sandbox' if self.use_sandbox else 'Production'} mode\")\n",
    "        \n",
    "    def get_account_balance(self):\n",
    "        \"\"\"Get the available MTurk account balance\"\"\"\n",
    "        response = self.client.get_account_balance()\n",
    "        balance = response['AvailableBalance']\n",
    "        print(f\"MTurk account balance: ${balance}\")\n",
    "        return float(balance)\n",
    "    \n",
    "    def create_hit_with_survey_link(self, survey_link, hit_config=None):\n",
    "        \"\"\"\n",
    "        Create an MTurk HIT with a link to a Qualtrics survey\n",
    "        \n",
    "        Args:\n",
    "            survey_link (str): URL to the Qualtrics survey\n",
    "            hit_config (dict, optional): Custom configuration for the HIT\n",
    "            \n",
    "        Returns:\n",
    "            str: HIT ID\n",
    "        \"\"\"\n",
    "        print(\"Creating MTurk HIT with survey link\")\n",
    "        \n",
    "        # Default HIT configuration\n",
    "        if not hit_config:\n",
    "            hit_config = {\n",
    "                'Title': 'Complete a short survey',\n",
    "                'Description': 'We need your input for a quick survey that should take less than 10 minutes',\n",
    "                'Keywords': 'survey, research, opinion, feedback',\n",
    "                'Reward': '0.50',\n",
    "                'MaxAssignments': 10,\n",
    "                'LifetimeInSeconds': 86400,  # 1 day\n",
    "                'AssignmentDurationInSeconds': 1800,  # 30 minutes\n",
    "                'AutoApprovalDelayInSeconds': 86400,  # 1 day\n",
    "                'QualificationRequirements': []\n",
    "            }\n",
    "        \n",
    "        # Create the HTML question with the survey link\n",
    "        question_html = f\"\"\"\n",
    "        <HTMLQuestion xmlns=\"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd\">\n",
    "            <HTMLContent><![CDATA[\n",
    "                <!DOCTYPE html>\n",
    "                <html>\n",
    "                <head>\n",
    "                    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
    "                    <script type='text/javascript' src='https://s3.amazonaws.com/mturk-public/externalHIT_v1.js'></script>\n",
    "                </head>\n",
    "                <body>\n",
    "                    <form name='mturk_form' method='post' id='mturk_form' action='https://www.mturk.com/mturk/externalSubmit'>\n",
    "                        <input type='hidden' value='' name='assignmentId' id='assignmentId'/>\n",
    "                        <h1>Survey Task</h1>\n",
    "                        <p>Please complete the survey at the following link:</p>\n",
    "                        <p><a href='{survey_link}' target='_blank'>{survey_link}</a></p>\n",
    "                        <p>After completing the survey, you will receive a completion code. Enter the code below:</p>\n",
    "                        <p><input type='text' name='completion_code' id='completion_code' size='40'/></p>\n",
    "                        <p><input type='submit' id='submitButton' value='Submit' /></p>\n",
    "                    </form>\n",
    "                    <script language='Javascript'>\n",
    "                        turkSetAssignmentID();\n",
    "                    </script>\n",
    "                </body>\n",
    "                </html>\n",
    "            ]]></HTMLContent>\n",
    "            <FrameHeight>600</FrameHeight>\n",
    "        </HTMLQuestion>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the HIT\n",
    "        response = self.client.create_hit(\n",
    "            Title=hit_config['Title'],\n",
    "            Description=hit_config['Description'],\n",
    "            Keywords=hit_config['Keywords'],\n",
    "            Reward=hit_config['Reward'],\n",
    "            MaxAssignments=hit_config['MaxAssignments'],\n",
    "            LifetimeInSeconds=hit_config['LifetimeInSeconds'],\n",
    "            AssignmentDurationInSeconds=hit_config['AssignmentDurationInSeconds'],\n",
    "            AutoApprovalDelayInSeconds=hit_config['AutoApprovalDelayInSeconds'],\n",
    "            Question=question_html,\n",
    "            QualificationRequirements=hit_config['QualificationRequirements']\n",
    "        )\n",
    "        \n",
    "        hit_id = response['HIT']['HITId']\n",
    "        hit_type_id = response['HIT']['HITTypeId']\n",
    "        \n",
    "        print(f\"HIT created successfully with ID: {hit_id}\")\n",
    "        \n",
    "        # Print the HIT URL\n",
    "        if self.use_sandbox:\n",
    "            worker_url = f\"https://workersandbox.mturk.com/mturk/preview?groupId={hit_type_id}\"\n",
    "        else:\n",
    "            worker_url = f\"https://worker.mturk.com/mturk/preview?groupId={hit_type_id}\"\n",
    "            \n",
    "        print(f\"Workers can access the HIT at: {worker_url}\")\n",
    "        \n",
    "        return hit_id\n",
    "    \n",
    "    def get_hit_assignments(self, hit_id):\n",
    "        \"\"\"\n",
    "        Get all assignments for a HIT\n",
    "        \n",
    "        Args:\n",
    "            hit_id (str): ID of the HIT\n",
    "            \n",
    "        Returns:\n",
    "            list: List of assignment dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"Getting assignments for HIT: {hit_id}\")\n",
    "        \n",
    "        # List to store all assignments\n",
    "        all_assignments = []\n",
    "        \n",
    "        # Get assignments with pagination\n",
    "        next_token = None\n",
    "        \n",
    "        while True:\n",
    "            if next_token:\n",
    "                response = self.client.list_assignments_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    NextToken=next_token,\n",
    "                    MaxResults=100\n",
    "                )\n",
    "            else:\n",
    "                response = self.client.list_assignments_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    MaxResults=100\n",
    "                )\n",
    "            \n",
    "            all_assignments.extend(response['Assignments'])\n",
    "            \n",
    "            if 'NextToken' in response:\n",
    "                next_token = response['NextToken']\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(all_assignments)} assignments\")\n",
    "        return all_assignments\n",
    "    \n",
    "    def approve_assignments(self, assignments, feedback=None):\n",
    "        \"\"\"\n",
    "        Approve multiple assignments\n",
    "        \n",
    "        Args:\n",
    "            assignments (list): List of assignment dictionaries or IDs\n",
    "            feedback (str, optional): Feedback to workers\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of successfully approved assignments\n",
    "        \"\"\"\n",
    "        approved_count = 0\n",
    "        \n",
    "        for assignment in assignments:\n",
    "            # Extract assignment ID if a dictionary was provided\n",
    "            assignment_id = assignment['AssignmentId'] if isinstance(assignment, dict) else assignment\n",
    "            \n",
    "            try:\n",
    "                self.client.approve_assignment(\n",
    "                    AssignmentId=assignment_id,\n",
    "                    RequesterFeedback=feedback if feedback else \"Thank you for your participation!\"\n",
    "                )\n",
    "                approved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error approving assignment {assignment_id}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Successfully approved {approved_count} assignments\")\n",
    "        return approved_count\n",
    "    \n",
    "    def delete_hit(self, hit_id):\n",
    "        \"\"\"\n",
    "        Delete a HIT\n",
    "        \n",
    "        Args:\n",
    "            hit_id (str): ID of the HIT to delete\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get the HIT status\n",
    "            hit = self.client.get_hit(HITId=hit_id)\n",
    "            status = hit['HIT']['HITStatus']\n",
    "            \n",
    "            # If the HIT is reviewable, dispose of it\n",
    "            if status == 'Reviewable':\n",
    "                self.client.delete_hit(HITId=hit_id)\n",
    "                print(f\"HIT {hit_id} deleted successfully\")\n",
    "                return True\n",
    "            \n",
    "            # If the HIT is assignable, expire it first then delete it\n",
    "            elif status == 'Assignable':\n",
    "                self.client.update_expiration_for_hit(\n",
    "                    HITId=hit_id,\n",
    "                    ExpireAt=datetime(2015, 1, 1)  # Set to a past date to expire immediately\n",
    "                )\n",
    "                time.sleep(1)  # Give time for the HIT to update\n",
    "                self.client.delete_hit(HITId=hit_id)\n",
    "                print(f\"HIT {hit_id} expired and deleted successfully\")\n",
    "                return True\n",
    "                \n",
    "            else:\n",
    "                print(f\"Cannot delete HIT {hit_id}, status is {status}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting HIT {hit_id}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional\n",
    "\n",
    "class QualtricsAndMTurkAutomation:\n",
    "    def __init__(self, mturk_client: Optional[MTurkClient] = None):\n",
    "        load_dotenv()\n",
    "        self.qualtrics = QualtricsClient()\n",
    "        self.mturk     = mturk_client or MTurkClient()\n",
    "\n",
    "    def run(self, survey_payload: dict, hit_config: dict) -> dict:\n",
    "        survey_id = self.qualtrics.create_survey(\n",
    "            survey_name=survey_payload[\"SurveyName\"],\n",
    "            survey_template=survey_payload\n",
    "        )\n",
    "\n",
    "        questions = []\n",
    "        for qid, qobj in survey_payload[\"Questions\"].items():\n",
    "            num = qid.lstrip(\"Q\")\n",
    "            real_qid = f\"QID{num}\"\n",
    "            questions.append({\n",
    "                \"question_id\":   real_qid,           \n",
    "                \"question_text\": qobj[\"QuestionText\"], \n",
    "                \"QuestionID\":    real_qid,          \n",
    "                \"QuestionText\":  qobj[\"QuestionText\"],\n",
    "                \"QuestionType\":  qobj[\"QuestionType\"],\n",
    "                \"Selector\":      qobj[\"Selector\"],\n",
    "                \"SubSelector\":   qobj[\"SubSelector\"],\n",
    "                \"Choices\":       qobj[\"Choices\"]\n",
    "            })\n",
    "        self.qualtrics.add_questions(survey_id, questions)\n",
    "\n",
    "\n",
    "        comp_qid = f\"QID{len(questions)+1}\"\n",
    "        self.qualtrics.add_questions(survey_id, [{\n",
    "            \"question_id\":   comp_qid,\n",
    "            \"question_text\": (\n",
    "                \"Thank you for completing the survey!\\n\"\n",
    "                \"Your completion code is: ${e://Field/ResponseID}\"\n",
    "            ),\n",
    "            \"QuestionID\":    comp_qid,\n",
    "            \"QuestionText\":  (\n",
    "                \"Thank you for completing the survey!\\n\"\n",
    "                \"Your completion code is: ${e://Field/ResponseID}\"\n",
    "            ),\n",
    "            \"QuestionType\":  \"DB\",   # Descriptive Text\n",
    "            \"Selector\":      \"TB\"    # Text/Graphic Block\n",
    "        }])\n",
    "\n",
    "        self.qualtrics.activate_survey(survey_id)\n",
    "\n",
    "        survey_link = self.qualtrics.create_distribution_link(survey_id)\n",
    "\n",
    "        hit_id = self.mturk.create_hit_with_survey_link(survey_link, hit_config)\n",
    "\n",
    "        return {\n",
    "            \"survey_id\":   survey_id,\n",
    "            \"survey_link\": survey_link,\n",
    "            \"hit_id\":      hit_id\n",
    "        }\n",
    "    def collect_and_process_results(self, survey_id, hit_id, auto_approve=True):\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "         \n",
    "            responses_df = self.qualtrics.get_survey_responses(survey_id)\n",
    "            results['responses'] = responses_df\n",
    "            \n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            csv_filename = f\"survey_responses_{timestamp}.csv\"\n",
    "            responses_df.to_csv(csv_filename, index=False)\n",
    "            results['csv_filename'] = csv_filename\n",
    "            \n",
    "            print(f\"Saved {len(responses_df)} responses to {csv_filename}\")\n",
    "            \n",
    "            assignments = self.mturk.get_hit_assignments(hit_id)\n",
    "            results['assignments'] = assignments\n",
    "            \n",
    "            if auto_approve and assignments:\n",
    "                approved_count = self.mturk.approve_assignments(assignments)\n",
    "                results['approved_count'] = approved_count\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting results: {str(e)}\")\n",
    "            return results\n",
    "\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # survey_to_process = \"\"\"\n",
    "    # Topic: The Theory of Planned Behavior Survey\n",
    "    # Questions:\n",
    "    # 1. I intend to purchase organic food in the next month. (1=Strongly disagree; 7=Strongly agree)\n",
    "    # 2. Buying organic food is beneficial to my health. (1=Strongly disagree; 7=Strongly agree)\n",
    "    # 3. I feel confident in my ability to purchase organic food if I want to. (1=Strongly disagree; 7=Strongly agree)\n",
    "    # 4. People who are important to me think I should purchase organic food. (1=Strongly disagree; 7=Strongly agree)\n",
    "    # 5. The decision to buy organic food is entirely up to me. (1=Strongly disagree; 7=Strongly agree)\n",
    "    # \"\"\"\n",
    "    print(\"========================================\")\n",
    "    print(\"âš ï¸  INPUT REQUIREMENTS:\")\n",
    "    print(\"- You must include a line starting with 'Topic:'\")\n",
    "    print(\"- You must include at least one line starting with 'Questions:'\")\n",
    "    print(\"Otherwise, the survey cannot be processed.\")\n",
    "    print(\"========================================\")\n",
    "    survey_to_process = input(\"Please enter the Survey content: \")\n",
    "\n",
    "    survey_dict = asyncio.run(flow.kickoff_async(inputs={\n",
    "        'survey_text': survey_to_process\n",
    "    }))\n",
    "\n",
    "    annotated = survey_dict['original_with_comments']\n",
    "    revised   = survey_dict['revised_survey'] \n",
    "\n",
    "    survey_dict = asyncio.run(flow.kickoff_async(inputs={\n",
    "        'survey_text': survey_to_process\n",
    "    }))\n",
    "\n",
    "    \n",
    "    qualtrics_payload = survey_dict_to_qualtrics_payload(survey_dict)\n",
    "\n",
    "    hit_config = {\n",
    "        'Title': 'Complete a short survey on organic food',\n",
    "        'Description': survey_dict[\"revised_survey\"][\"purpose\"],\n",
    "        'Keywords': 'survey, research, feedback',\n",
    "        'Reward': '0.75',\n",
    "        'MaxAssignments': 20,\n",
    "        'LifetimeInSeconds': 86400,\n",
    "        'AssignmentDurationInSeconds': 1800,\n",
    "        'AutoApprovalDelayInSeconds': 86400,\n",
    "        'QualificationRequirements': []\n",
    "    }\n",
    "    \n",
    "    automation = QualtricsAndMTurkAutomation()\n",
    "    results = automation.run(qualtrics_payload, hit_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
